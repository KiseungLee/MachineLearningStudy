안녕하세요? 서울대학교 컴퓨터공학부의 김건희입니다. 인공지능의 기초 오늘은 강화 학습에 대해서 말씀을 드리도록 하겠습니다.
근래에 많은 분들이 강화 학습에 아주 관심이 많다고 생각하는데요.
가장 큰 이유가 얼마 전 알파고 혁명 때 알파고의 알고리즘이 강화 학습을 기반으로 만들어졌다는 게 알려지면서 많은 분들이 관심을 갖게 되었고요.
그 외에 게임을 하는 인공지능, 그러니까 여러분이 들어보셨겠지만 스타크래프트라는 게임 인공지능을 만드는 연구가 많이 진행되고 있다고 알고 계신데요.
그것도 강화 학습을 기반으로 해서 만들어지고 있다. 이렇게 이해하시면 되겠습니다.
본 강의를 소개시켜드리기에 앞서서 말씀을 드리고자 하는 내용은 여러분이 만약에 강화학습을 정말 잘 알고 싶다고 하면 지금 보여드리는 강의를 한번 들어보시는 게 좋을 수 있습니다.
David Silver라는 분이 university of college london에서 강의한 내용이고요.
David Silver는 아시는 분은 아시겠지만 그때 알파고 때 핵심, 그러니까 전체를 설계한 분으로 소개됐고 같이 참여도 하신 분으로 알려져 있습니다.
그분이 영국 대학교에서 강의를 열었는데 그 강의한 내용을 촬영한 내용입니다. 그래서 여러분이 동영상도 볼 수 있고요. 그리고 lecture note도 같이 다운을 받을 수 있습니다.
그런데 주의하셔야 할 게 강화 학습이라는 주제 자체가 좀 어려운 주제이기 때문에 대학원 정도 수준의 일반인들이 들으시기에는 어려울 수 있다는 게 단점이 될 수 있고요.
그리고 영어로 진행된 강의이기 때문에 영어에도 익숙하셔야 들을 수 있는 좋은 강의입니다.
하지만 전 세계에서 열리는 수많은 강의들 중에 가장 뛰어난 강의라고 꼽히고 있기 때문에 혹시 강화 학습을 깊게 알아보고 싶으신 분들은 꼭 한번 들어보시는 걸 추천해 드리고자 합니다.
그리고 조금 전에 말씀드린 강의도 마찬가지로 이 책을 기반으로 하고 있습니다. 「Reinforcement Learning: An Introduction」이고요.
이 책은 아주 자주 계속해서 업데이트되는 책이고요. 여러분이 서점을 통해서 구입이 가능하고요.
그리고 저자들이 온라인을 통해서 드래프트를 올린 것도 여러분이 받아볼 수 있기 때문에 미완성 책의 경우에는 여러분이 PDF를 받아서 볼 수 있습니다.
그래서 이 책도 마찬가지로 여러분이 강화 학습을 깊게 공부하고 싶다. 이러신 분들은 꼭 한번 찾아보시는 게 좋을 것 같습니다.
그러면 이제 강화 학습에 대해서 말씀을 드리도록 하겠습니다. 강화 학습은 인공지능의 아주 핵심 분야 중의 하나고요. 그러니까 인공지능 에이전트를 학습시키는 과정 중의 하나입니다.
그런데 어떻게 학습하느냐고 하면 이 에이전트 보고 어떤 행동을 하게 한 다음에 그 행동이 잘 했으면 보상을 하고요. 그다음에 잘 못 했으면 벌을 주는 그런 식으로 학습하는 과정입니다.
그러니까 어떤 에이전트에게 ‘뭘 어떻게 하라.’ 이거를 직접적으로 가르쳐주는 게 아니라 그냥 스스로 할 수 있게 한 다음에 그 결과를 놓고 reward 혹은 punishment를 주는 그런 경우라고 할 수 있습니다.
그래서 사람에 대한 학습을 예로 들자면 어린 아이가 빨간불일 때는 차도를 건너면 안 되고 파란불일 때 건너야 하는데 엄마가 애한테 ‘파란불일 때 건너고 빨간불일 때는 건너지 마.’
이런 식으로 가르쳐줄 수도 있지만 또 다른 방식으로는 애가 자기 멋대로 하도록 놔두는 것이죠.
그래서 애가 만약에 파란불일 때 건너면 보상을 해주고요. 빨간불일 때 건너면 벌을 주고, 그걸 반복하다보면 애가 스스로 ‘빨간불일 때는 건너지 말아야겠구나.’ 이걸 학습할 수 있도록 하는 것이죠.
한편으로 보면 비효율적인 학습방법이라고 할 수도 있습니다. 왜냐하면 직접적으로 알려주는 게 아니라
얘 보고 Trial&Error를 하도록 반복한 다음에 저희는 보상과 punishment로 에이전트를 학습하는 거기 때문에
많은 경우에 강화 학습을 통해서 학습을 하면 그 에이전트를 쉽게 학습할 수 없다는 단점은 있습니다.
대신에 강화 학습은 언제 활용이 되느냐면 저희가 그때마다 룰을 일일이 가르쳐주는 게 매우 어려운 환경에서는 강화 학습이 유리할 수 있습니다.
그러니까 게임 같은 인공지능에서는 저희가 룰을 몇 개는 만들어서 알려줄 수 있지만 많은 경우에는 그때마다 임기응변으로 해야 할 상황이 많고요.
그렇기 때문에 그런 환경에서는 저희가 어떻게 일일이 ‘어떻게 해라. 어떻게 해라.’ 학습을 시켜주기보다는 스스로 수많은 게임을 통해서 학습하도록 가르친다고 볼 수 있고요.
마찬가지로 사람도 게임을 잘하게 되는 과정을 보면 이런 강화 학습의 아이디어로 점점 잘해진다고 볼 수도 있습니다. 그래서 학습을 어떻게 하느냐? 보상 함수를 통해서 학습을 하고요.
그다음에 fills in the details. 그러니까 자세한 내용은 에이전트가 스스로 배워야 하는 겁니다. 제가 일부러 가르쳐주는 건 아니고요.
그리고 Feedback, 그러니까 보상 혹은 벌이라는 게 내가 어떤 행동을 했을 때 바로바로 주어질 수도 있지만 많은 경우에는 한참 후에 주어질 수 있습니다.
바둑 같은 예죠. 바둑도 한 번 수를 둘 때마다 저희가 보상을 해주는 게 아니라 끝까지 가보고 게임을 끝까지 해봐야지 ‘졌구나.’ 혹은 ‘이겼구나.’ 이를 통해서 강화 학습을 적용할 수 있겠죠.
그리고 타이밍이라는 게 굉장히 중요합니다.
그러니까 처음에는 강화 학습 에이전트가 거의 아는 게 없다가 얘가 점점 알아가는 게 많아질 텐데 그런 상황에서는 어떤 순서로 얘를 학습시키느냐도 매우 중요합니다.
그래서 non i.i.d라는 거는 i.i.d는 independent identical distribute이란 뜻인데, 그러니까 저희의 학습 데이터가 서로 다 독립 사건이 아니라는 거고요.
그리고 그 학습 데이터가 똑같은 확률분포에서 주어지지 않았다. 그렇게 볼 수 있습니다.
그리고 에이전트가 특정 스텝에서 어떤 액션을 취했느냐가 다음에 얻을 데이터가 어떤 형태로 올 것인지 결정하는 경우가 많습니다.
그래서 일단 학습을 시키는, 그러니까 선생님 입장에서는 시간을 많이 save할 수 있지만
대신에 배우는 자가 더 많은 Trial&Error를 통해서 스스로 배워야 하는 그런 장단점이 있는 학습방식이라고 보시면 되겠습니다.
이 강화 학습 부분은 좀 전에 말씀드렸다시피 advanced, 고급 과정이라고 볼 수 있습니다.
그래서 여러분이 강화 학습 강의를 잘 이해하고 싶은 경우에는 확률통계라든지 여러 가지 선형대수에 대한 기본적인 지식을 선수과목으로 갖출 필요가 있고요.
혹시라도 하나하나 설명해 나가는 과정에서 이해가 안 되는 부분이 있다고 하면 찾아보면서 진행하셔야 하지 않을까 생각합니다.
그래서 기본적인 강화 학습 모델에 대해서 도식화하면 이것과 같습니다. 여기 보시다시피 에이전트가 있고요. 에이전트는 그 주변 환경이 있습니다. 환경이 있고요.
이 주변 환경과의 인터랙션을 통해서 에이전트가 점점 학습해 나가는 과정이라고 보시면 되겠고요.
그래서 강화 학습의 목적이라고 하면 에이전트를 제어하는 정책을 찾는 건데 어떤 정책을 찾느냐?
에이전트가 최대의 보상을 얻을 수 있는 control policy, 행동 혹은 제어 정책을 찾는 것이라고 보시면 되겠고요.
policy라는 단어에 대한 구체적인 정의는 다음 슬라이드에서 다시 말씀드리도록 하겠습니다.
그래서 에이전트 관점에서는 현재 에이전트가 어떤 상태에 있다가 그다음에 주어진 상태에서 내가 취할 수 있는 액션 셋이 여러 개 있겠죠.
액션 셋에 액션이 여러 개가 있을 텐데, 그 중에 하나의 액션을 선택해서 수행을 합니다. 그러면 그 액션을 통해서 이 에이전트의 state는 다음 state로 변경이 되고요.
다음 state로 변경이 됨과 동시에 지금 직전에 한 행동에 대한 reward가 주어집니다.
그러니까 St라는 상태에서 At라는 액션을 취했을 때 St+1이라는 state로 바뀌면서 reward가 얼마나 주어지느냐? 그거에 대한 내용이라고 보시면 되겠고요.
이 과정을 반복함으로써, reward를 계속 관측함으로써 ‘내가 어떤 상태에서는 어떤 액션을 취해야 되겠구나.’ 그걸 스스로 배울 수 있도록 한다고 보시면 되겠습니다.
여기서 가장 중요한 값 중의 하나가 reward입니다. 그러니까 결국에는 에이전트를 학습시키는 게 이 reward라는 scaler, 그러니까 하나의 값을 통해서 컨트롤하게 되기 때문에
이 reward를 어떻게 정의하고 어떻게 주느냐? 이게 참 쉽지 않은 문제고요. 실제로 강화 학습을 적용할 때도 reward를 어떻게 정의하느냐? 그게 참 어려운 일입니다.
그래서 하나하나 중요한 정의에 대해서 말씀을 드리도록 하겠습니다. 에이전트 같은 경우에는 이 3가지를 다 가지고 있는 경우도 있고 일부만 가지고 있는 경우도 있습니다.
그래서 하나하나씩 보면 policy. policy라는 건 뭐냐면 에이전트의 behavior function이다. 그러니까 에이전트가 어떤 상태에서 어떤 액션을 할지 정해주는 함수라고 보시면 되겠고요.
value function이라는 건 뭐냐면 내가 어떤 state에 갔을 때 그 state가 얼마나 좋은지, 그러니까 내가 reward를 계속 maximizing하는 방향으로 행동을 할 텐데
내가 어떤 state에서 어떤 state로 갈 때 그 state가 얼마나 좋은지 나타내는 함수를 value function이라고 하고요.
그다음에 model이라고 하는 건 에이전트가 환경을 어떻게 표현하고 있는지 그거에 대한 모든 것을 모델이라고 통칭합니다.
그러니까 소개시켜드리는 단어 하나하나가 다른 콘텍스트에서는 다른 의미로 쓰이는데 적어도 강화 학습에서는 이런 의미로 쓰인다는 것을 여러분이 확실히 이해하셔야 할 필요가 있고요.
그래서 policy, value function, model 이게 강화 학습에서 제일 중요한 컨퍼런트이고요. 그래서 이 3개의 단어는 강의 내내 계속 나올 것입니다.
이것에 대해서는 확실히 이해할 필요가 있고 지금까지는 개념적으로 말씀드렸다고 하면 하나하나씩 구체적인 정의에 대해서 말씀을 드리도록 하겠습니다.
그래서 policy는 함수입니다. 함수인데 state로부터 액션으로의 함수입니다. 그래서 크게 보면 Deterministic policy가 있고, Stochastic policy가 있습니다.
그러니까 Deterministic이라는 건 뭐냐면 내가 어떤 상태에서 어떤 액션을 할지 정의해놓은 겁니다. 하나하나씩 정의한 거고요.
Stochastic이라는 거는 어떤 state에서 어떤 액션을 취할지 확률분포로 주어지는 겁니다. 그러니까 어떤 액션을 취할 확률이 높긴 하지만 항상 그 액션을 취하는 건 아니다.
그렇게 이해하면 되겠습니다. 그리고 policy는 통상적으로 π라는 notation으로 많이 쓰고요. 그래서 π는 함수인데 s를 받아서 a를 아웃풋으로 내는 함수다. 그렇게 이해하시면 되겠고요.
Stochastic policy 같은 경우에는 좀 전에 말씀드렸다시피 어떤 state에서 어떤 액션을 취할지가 확률분포로 주어지는 거고요.
확률분포이기 때문에 확률분포를 값 하나로 요약하는 게 expectation입니다. 그래서 expectation은 기댓값이라고 정의가 되는데요.
그래서 혹시라도 확률분포에 대해서 약간 배경지식이 적다고 하면 expectation의 의미에 대해서 한번 알아보시는 게, 찾아보시고 이 강의를 다시 듣는 게 좋을 수도 있습니다.
간단히 말씀드리면 어떤 state에서 어떤 액션을 취할지에 대한 확률분포라고 이해하시면 되겠습니다.
그다음에 중요한 정의 중의 하나입니다. value function인데요. 가치 함수라고 한국말로 얘기하고요. 가치 함수는 미래의 보상에 대한 예측입니다. 예측 값이고요.
일단은 이런 식으로 notation을 씁니다. V라는 notation을 주로 쓰고요, value function을 위해서.
이 value function이라는 건 어떻게 정의가 되냐면 만약에 policy가 주어졌을 때 내가 만약에 π라는 policy를 따라서 행동한다고 했을 때 S라는 state의 value가 얼마냐?
이거를 정의한다고 보시면 되겠습니다. 그래서 나중에 가면 value function이 크게 2종류가 있습니다. V로 쓰는 경우와 Q로 쓰는 경우가 있는데 V는 state에 대한 함수고요.
Q는 state와 액션에 대한 함수입니다. 그래서 Q는 나중에 보시면 되겠고요. V부터 이해하시면 된다고 보시면 되겠습니다. 그래서 V는 어떤 policy를 따른다고 했을 때 어떤 state의 value를 나타낸다고.
그런데 저희가 이렇게 π라고 서브스크립트, 그러니까 아래첨자로 써주는 이유는 만약에 policy가 바뀌면 어떤 state의 값이 자동으로 바뀌게 됩니다.
그래서 아래첨자로 π를 썼다는 건 얘가 π라는 policy를 따라갈 때의 value function이 이렇게 주어진다고 보시면 되겠고요. 그래서 정의대로 보시면 제가 S라는 state에 있습니다.
그러니까 타임 스텝 t에 제가 S라는 state에 있을 때 얘가 계속 액션 그리고 state 변화를 반복하게 되겠죠. 반복하게 되는데 일종의 기댓값입니다.
기댓값인데 미래에 대한 reward의 기댓값이고요. 현재 state가 S일 때 내가 미래에 취할 수 있는 reward에 대한 기댓값이라고 보시면 되겠습니다.
그리고 여기 보시다시피 Rt+1은 t+1 스텝에서의 reward, Rt+2는 t+2 스텝에서의 reward, 이렇게 보시면 되겠고 이 γ라는 건 discount function이라고 합니다.
그러니까 먼 미래에 대한 reward는 γ만큼 discount를 해준다는 거죠. 그래서 γ는 많은 경우에 1보다 작은 값을 줍니다.
그래서 만약에 γ²이 되면 훨씬 더 작은 값이 되고, γ³이 되면 훨씬 더 작은 값이 되고요. 그래서 멀면 멀수록 그거에 대한 reward는 discount한 값이라고 보시면 되겠습니다.
그리고 model이라고 말씀드린 게 있었죠. model은 크게 환경에 대한 representation이라고 아까 정의를 했는데 크게 보면 2가지 구성요소로 이루어져 있습니다.
하나는 P이고요, 하나는 R입니다. P라는 건 뭐냐 하면 마찬가지로 제가 S라는 state에 있습니다. S라는 state에 있을 때 a라는 액션을 취하면 S`이라는 state로 갈 확률을 나타냅니다.
그러니까 오른쪽에 기술했다시피 제가 t 스텝의 S라는 state에 있고 그때 내가 만약에 a라는 액션을 취했을 때 그다음 state로 S`으로 갈 확률, 그거를 P라는 notation으로 정의하게 되고요.
그리고 R이라는 건 뭐냐면 제가 S라는 state에 있을 때 a라는 액션을 취하면 그때 얻는 reward다. 그래서 예를 들면 제가 t 스텝에 a라는 액션을 취했습니다.
그리고 t state에서 S라는 state에 있었습니다. 그때 a라는 액션을 취했을 때 그다음 state로 가면서 그때 얻는 reward에 대한 확률분포라고 보시면 되겠습니다.
그래서 강화 학습에서 모델이라고 하면 P랑 R을 지칭한다. 이렇게 보시면 되겠습니다.
그래서 P는 state transition, 그리고 R은 reward에 대한 확률분포다. 이렇게 이해하시면 되겠습니다.
그래서 예제와 함께 이해를 해보도록 하죠. 그래서 이거는 기본적으로 미로에 있습니다. 미로에 있는데 start부터 시작해서 goal까지 가는 게 에이전트의 목적입니다.
그런데 저희가 기대하는 거는 최단거리로 goal까지 가는 걸 기대하는 것입니다. 그럴 경우에는 저희가 reward를 어떻게 주느냐면 얘가 한번 타임 스텝을 할 때마다 reward를 -1로 줍니다.
그러니까 이거는 무슨 뜻이냐 하면 내가 쓸데없는 행동을 하면 할수록 -1을 더 많이 받기 때문에 이 reward 함수를 활용한다면 우리가 최단거리로 goal까지 갈 수 있겠죠.
그리고 액션이라는 건 N, E, S, W. 그러니까 내가 북쪽으로 갈 수도 있고 동쪽으로 갈 수도 있고 서쪽으로 갈 수도 있고 W로 갈 수 있고,
그러니까 각각의 location에서 내가 취할 수 있는 액션은 총4가지입니다. 그리고 여기서 state는 어떻게 정의했냐면 에이전트가 현재 어느 위치에 있는지를 state로 정했다고 보시면 되겠습니다.
그래서 initial state는 스타트 포인트가 되겠고요. goal state는 goal 포지션이 되겠죠.
그래서 이런 식으로 에이전트를 학습하다 보면 결국에는 최단거리로 스타트부터 goal까지 가는 그런 에이전트가 학습되겠죠.
그래서 policy의 한 예제입니다. 여기서는 Deterministic policy를 예제로 보여드리고 있고요.
아까 policy가 뭐라고 말씀드렸죠? policy라는 건 내가 어떤 state에서 어떤 액션을 취할지 나타내는 함수라고 말씀드렸습니다. 그래서 notation으로는 π를 썼고요.
그래서 왼쪽 그림을 보시면 각각의 state마다 화살표가 있습니다.
그래서 화살표의 이름이 뭐냐면 이 에이전트가 만약에 이 위치에 있다고 하면 이 state에 있을 때는 북으로 가는 액션을 취해라. 그게 policy입니다.
그래서 여기 지금 왼쪽 그림에서 주어져 있다시피 모든 state에서 어떤 액션을 취해야 하는지 화살표로 표현한 것이죠. 그래서 이거는 하나의 policy의 예제라고 이해하시면 되겠습니다.
그리고 value function은 아까도 말씀드렸다시피 내가 어떤 policy를 취하고 있을 때 어떤 state의 값입니다. 그러니까 그 state가 얼마나 가치가 있는지 나타내는 값이죠.
그래서 오른쪽의 그림을 보시면 value function의 한 예를 보여주고 있습니다. 그러니까 각 state마다 그 state의 value가 얼마인지 나타내고 있는 것이죠.
그래서 많은 경우에 value function이랑 policy랑 되게 관련이 깊습니다. 만약에 여러분이 이 위치에 있다고 한번 가정을 해보죠.
이 위치에 있다고 했을 때 왼쪽 state의 value는 -16이고요. 아래쪽 state는 -16이고요. 위쪽에 있는 state는 -14입니다. 이럴 경우에는 어떤 policy를 취하게 될까요?
당연히 위쪽으로 가는 policy를 취하겠죠. 왜냐하면 그쪽으로 가는 게 그 state의 value가 가장 최대이기 때문입니다.
그래서 이 value function이라는 건 각각의 state가 얼마나 value가 있는지, 얼마나 가치가 있는지를 나타내고요.
이 예제에서 볼 수 있다시피 goal이랑 가까우면 가까울수록 value값이 큽니다.
그러니까 goal 바로 옆에 있는 state 같은 경우에 -1이이죠. 그리고 start에 가까울수록 value function값이 작습니다.
그러니까 이런 식으로 value function이 주어져 있다고 하면 에이전트는 value function이 커지는 방향으로 계속 이동을 하게 될 테고요.
그러다 보면 value function만으로도 에이전트를 제어할 수 있는 상황이 오게 됩니다.
그래서 나중에도 계속 말씀드리겠지만 value function이랑 policy랑은 되게 밀접한 관계가 있다. 이렇게 보시면 되겠고요.
많은 경우에 하나만 알고 있어도 다른 하나를 유도할 수 있고요. 그에 대해서는 좀 더 자세히 다루도록 하겠습니다.
그리고 model에 대한 내용이죠. model은 아까도 말씀드렸다시피 크게 P와 R이라는 2개의 정보가 있다고 말씀드렸습니다.
P는 state transition, 그러니까 S라는 state에 있을 때 a라는 액션을 취하면 S`이라는 state로 갈 확률을 나타내는 거고요.
reward라는 건 S라는 state에서 a라는 액션을 취했을 때 reward, 그거를 나타낸다고 할 수 있겠죠.
Grid layout이라는 게 여기서 transition model을 정의하고 있다고 보시면 되는데요.
그러니까 여기 흰 색으로 표현됐다는 건 뭐냐면 여기서 이쪽으로도 갈 수 있고 이쪽으로도 갈 수 있다는 것입니다.
그런데 여기서 북쪽으로는 못 가겠죠. 북쪽은 검은색이고 또 마찬가지로 왼쪽으로도 못 가겠죠. 왼쪽도 마찬가지로 검은색이니까.
그러니까 지금 이렇게 흰색으로 표현됐다는 것 자체가 implicit하게 transition model을 정의하고 있다. 이렇게 보시면 되겠고요.
그다음에 여기 지금 숫자들이 이렇게 -1로 채워져 있는데 이 -1이라는 게 reward function, 그러니까 보상 함수라고 이해하시면 되겠습니다.
그래서 저희가 문제 시작할 때 우리가 한번 move할 때마다 reward가 -1씩 주어진다고 했는데, 그거를 표현하면 이런 식으로 주어진다.
그래서 Maze Example에 대해서 저희가 문제 정의를 말씀드렸고 그다음에 policy의 예 하나 보여드렸고, value의 예, 그다음에 model의 예를 하나씩 보여드렸다고 이해하시면 되겠습니다.
그리고 중요한 개념 중의 하나가 Exploration and Exploitation입니다. 그러니까 탐험을 하는 것과 제가 알고 있는 정보를 활용하는 것이죠.
그래서 시작할 때 reinforcement learning, 강화학습을 시작할 때 말씀드렸다시피 그거는 기본적으로 Trial&Error를 반복함으로써 에이전트가 학습하는 과정을 보여드리고 있는데요.
많은 경우에는 제가 경험이 쌓이면 쌓일수록 제 경험상 ‘이런 액션을 취했을 때 reward가 많았으니까 이런 reward를 취해야지.’
이런 식으로 Exploitation, 알고 있는 지식을 활용하는 경우죠. 그런데 그것만 활용해서는 안 됩니다.
왜냐하면 제가 처음에 두 번째로 좋은 액션을 취해서 reward 1을 받았는데 원래 처음에 제일 좋은 액션을 취했으면 reward를 예를 들어서 10을 받을 수 있었는데 제가 미처 그걸 안 했다.
그럴 경우에는 제가 처음에 그걸 안 했으면 계속 그걸 안 하게 되는 그런 현상이 벌어질 수도 있지 않습니까?
그래서 많은 경우에는 Exploitation, 그러니까 내가 알고 있는 정보를 활용해서 reward를 maximizing하는 액션을 하는 것도 중요하지만,
일종의 random action을 취해서 ‘내가 이런 상황에서 어떤 액션을 취하면 이런 reward를 받는구나.’ 그런 경험을 쌓는 과정도 필요합니다.
그래서 강화 학습일 경우에는 Exploration이랑 Exploitation을 반복하고 이 둘 사이에 트레이드오프 관계가 존재한다. 이렇게 이해하시면 되겠고요.
이걸 우리가 일상생활의 예제로 한번 본다고 하면 우리가 식당을 결정할 때도 우리가 알고 있는 가장 맛있는 식당을 가는 경우도 있지만
우리가 가보지는 않았지만 한번 새로운 레스토랑을 시도해볼 수도 있겠죠. 그렇게 새로 시도해보는 걸 Exploration이라고 할 수 있고,
그리고 내가 현재까지 알고 있는 지식을 바탕으로 해서 제일 좋다고 생각되는 레스토랑을 가는 그 과정을 Exploitation 혹은 활용, 이용이라고 볼 수 있습니다.
그리고 또 하나 중요한 2가지 term입니다. 이거는 Prediction이랑 Control이라는 term인데요. Prediction은 말 그대로 일종의 예측이고, 한번 예측을 해본다. 이렇게 해석할 수 있고요.
Control은 ‘제어한다’ 이렇게 볼 수 있는데 reinforcement learning에서의 Prediction은 무슨 뜻이냐 하면 policy가 주어졌을 때 value function을 구하는 것, 그 작업을 Prediction이라고 합니다.
그러니까 어떤 policy π이라는 게 주어졌을 때 가치 함수, 그러니까 Vπ(S)를 구하는 것이죠. π가 주어졌을 때 Vπ(S)를 구하는 과정을 Prediction이라고 합니다.
그리고 Control이라는 건 뭐냐면 문제가 주어졌을 때 최적의 policy, 그러니까 reward를 최대한으로 할 수 있는 policy를 찾는 과정, 그걸 Control이라고 합니다.
그래서 Control은 제어다. 그거는 많은 분들이 아실 테고 제어는 다른 데서는 다른 의미로 쓰일 텐데 reinforcement learning에서는 이런 식으로 정의가 되었다고 이해할 필요가 있습니다.
그러면 Prediction과 policy의 예제를 한번 보여드리도록 하겠습니다. 여기서의 예제는 에이전트가 이 Grid 상 어딘가에 위치할 수 있습니다.
어딘가 위치할 수 있는데 만약에 에이전트가 A라는 위치에 도달했다고 해보십시오. A라는 위치에 도달하면 reward로 10을 받고, A`이라는 위치로 이동하게 됩니다.
그리고 만약에 에이전트가 B라는 위치에 있다고 했을 때는 reward를 5 받고요. B`이라는 데로 이동을 하게 됩니다.
예를 들어서 policy를 uniform random policy로 정했습니다.
uniform random은 제가 어떤 state든지 간에 북으로 갈 확률 그리고 남으로 갈 확률 그리고 동, 서로 갈 확률이 다 0.25로 동일하다는 것입니다.
그러니까 간단히 얘기하면 내가 어떤 state든지 간에 무작위로 위아래 왼쪽, 오른쪽을 간다는 것입니다.
그런 policy를 따라서 제가 따라간다고 했을 때 그때의 value function은 오른쪽과 같이 구할 수 있습니다. 이거는 어떻게 구하는지 다음 주에 배울 예정이고요.
현재로서는 그냥 policy가 문제가 주어지고 policy가 주어지면 이런 value function을 구할 수 있다는 것을 이해하시면 되겠습니다.
그래서 여기서 보시면 value function이 가장 높은 state가 여기 8,8입니다. 왜냐하면 내가 이쪽으로 가면 reward를 10으로 받고 A`으로 이동을 하게 되죠.
그래서 이 state, A라고 마크된 state로 가는 value function이 제일 높습니다. 그런데 8,8이라는 숫자가 어떻게 나왔냐? 이거는 다음 시간에 배우도록 하겠습니다.
그리고 이거는 policy, 이 슬라이드에 대해서는 policy에 대한 예제입니다. 그래서 여기 *라고 쓰여 있는 거는 optimal이라는 뜻입니다.
그래서 수많은 value function 중에서 optimal, 최적의 value function을 V*라고 하고요.
π*라는 건 뭐냐면 내가 취할 수 있는 수많은 policy 중에서 reward를 가장 maximizing하는 policy를 나타냅니다.
그러니까 여기 policy대로 따르자면 제가 만약에 이 state에 있다고 하면 오른쪽으로 가는 policy를 취하는 게 최적이라는 거죠. 왜냐?
이 state로 가면 +10이라는 reward를 받으면서 이 state로 이동을 하게 되죠. 그리고 내가 만약에 이 state에 있다고 하면 이렇게 가도 좋고 이렇게 가도 좋다는 겁니다.
그래서 만약에 내가 이쪽으로 이동했다고 하면 여기서는 반드시 북쪽으로 가는 게 좋고요. 그래서 오른쪽 그림은 policy에 대한 예제를 보여주고 있다고 이해하시면 되겠습니다.
그래서 마찬가지로 이런 optimal policy는 어떻게 구하느냐? 이거는 마찬가지로 다음 시간에 다룰 내용이고요.
일단 여기서는 이해하셔야 할 게 강화 학습에서 Prediction이 무엇을 의미하는지, Control이 무엇을 의미하는지 그거에 대한 각각의 예제를 보여드렸다고 이해하시면 되겠습니다.
그럼 지금까지 말씀드린 강화 학습의 기초 정의에 대해서는 여기까지 말씀드리고 다음 시간에 이어서 하도록 하겠습니다.
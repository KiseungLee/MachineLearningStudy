이제 강화 학습을 모델링하는 데에 있어서 가장 기본적인 모델인 마르코프 과정에 대해서 말씀드리도록 하겠습니다.
기본적으로 강화 학습은 Markov Decision Processes라고 마르코프 결정 과정이라는 모델을 기반으로 formulation이 됩니다.
그러니까 Markov Decision Processes까지 이해하는 데 있어서 가장 핵심이 되는, 그런 기본이 되는 마르코프 과정에 대해서 함께 알아보도록 하겠습니다.
아주 간단한 경우를 한번 보죠. 그러니까 하나의 결정을 하는 간단한 모델입니다.
제가 만약에 0이라는 state에 있고요. 0이라는 state에서 A랑 B라는 여러 개의 액션이 있습니다. 여기서는 2개죠. 2개의 액션이 있고 A라는 액션을 취하면 1이라는 state로 가고요.
B라는 액션으로 취하면 2라는 state로 갑니다. 그래서 reward는 각각 1이라는 reward와 2라는 reward를 받는 것이고요.
그래서 강화 학습의 목적인 ‘내가 이 reward를 maximizing, 최대화하는 선택을 policy, 선택을 찾겠다.’
이 경우라고 하면 그냥 0이라는 state에 있을 때 제가 B라는 액션을 취하면 된다. 이렇게 쉽게 이해할 수 있겠죠.
하지만 많은 경우에 이렇게 하나의 선택으로만 우리의 태스크가 모델링되는 경우는 없을 것입니다.
그래서 실제로는 아주 복잡한 여러 개의 state가 있고 또 이 여러 개 state 사이에서 내가 어떤 액션을 취하느냐에 따라서 어느 state로 갈지가 결정되고 또 reward도 어떻게 받고,
이런 수많은 훨씬 복잡한 과정을 겪게 되는데요. 이걸 전체적으로 포괄하는 모델을 Markov Decision Processes라고 합니다.
그래서 마르코프 결정 과정 이렇게 한국말로 말할 수 있고요. 그래서 MDP의 구체적인 정의는 다음 시간에 할 예정이고요.
MDP의 정의하는 데 있어서 가장 기본이 되는 마르코프 프로세스부터 하나씩 밟아 나가도록 하겠습니다.
가장 간단한 모델입니다. 마르코프 프로세스부터 시작하도록 하죠. 마르코프 프로세스는 크게 2가지 구성요소로 되어 있습니다.
2가지 구성요소로 되어 있고 S는 여기서 보시다시피 state의 집합입니다. state의 집합이고요. P라고 되어 있는 거는 저희가 이전 시간에 model이라고 얘기한 것 중의 하나였죠.
P는 뭐였냐 하면 state transition function입니다. P라는 함수는 S를 정의역으로 가지고 있고 S를 공역 혹은 치역으로 가지고 있습니다.
그러니까 state를 받아서 또 다른 state로 맵핑해주는 함수라고 이해하시면 되겠고요. 많은 경우에 이거는 확률분포 형태로 주어지게 됩니다.
그래서 내가 현재 state에서 다음 state로 갈 수 있는 확률을 P라는 매트릭스로 표현할 수 있겠죠. 그래서 만약에 우리가 n개의 state가 있다고 가정을 해보죠.
우리가 총 가능한 state가 n개다. 그러면 P라는 transition function은 n×n 매트릭스, n×n의 행렬로 표현할 수 있습니다.
그런데 이 위에서 보시면 memoryless random process라는 정의가 있습니다.
memoryless라는 거는 뒤에 나오겠지만 히스토리, 그러니까 히스토리가 현재 state 하나로 다 설명이 된다는 뜻이고요.
그러니까 현재 state만 알면 그 이전의 히스토리는 몰라도 된다. 그런 뜻입니다.
그래서 그런 의미로 memoryless라는 단어가 붙었고요. random process라는 거는 지금도 말씀드렸다시피 우리의 state transition이 어떤 확률분포를 따라서 가게 된다.
그거를 말씀드린다고 보시면 되겠습니다. 그래서 마르코프 프로세스의 정의를 보시면 두 번째 줄을 보시면 되겠죠. 그래서 random state의 연속이고요.
대신에 중요한 건 Markov property를 가정하고 state transition을 진행한다고 보시면 되겠습니다. 그래서 과연 Markov property가 뭐냐? 그거에 대한 정의가 바로 밑에 주어져 있는데요.
state라는 건 그 히스토리로부터 주어진 모든 정보를 다 가지고 있다. 이런 뜻입니다. 그래서 확률분포 수식으로 나타내면 이와 같이 표현할 수 있는데요.
그러니까 현재 state가 주어졌을 때 그다음 state, 어디로 갈지에 대한 확률분포라는 것과 기존에 내가 어느 state를 방문했는지 모든 히스토리가 주어졌을 때
다음 state에 어디로 갈지의 확률이 같다는 것입니다.
그러니까 현재 state까지 오는데 어떤 히스토리를 겪었든지 간에 현재 state만 알면 다음 state가 어디로 갈지에 대한 확률분포가 정해진다는 것입니다.
또 다른 얘기로 말씀을 드리면 현재 state가 주어지면 이 현재 state의 미래 일과 과거의 일이 서로 독립이다. 이렇게 이해할 수도 있습니다.
그래서 이거는 예제입니다. 하나의 예제이고요. 여기서 보시면 동그라미로 표현된 게 state이고요. 네모로 표현되어 있는 state가 있는데 이거는 터미널, 종료 state입니다.
그러니까 여기에 도달하는 순간 더 이상 갈 수 있는 곳이 없죠. 그래서 하나의 예제를 들어보시면 제가 만약에 C1이라는 state에 있다고 하면
0.5의 확률로 C2라는 state로 가거나 혹은 0.5의 확률로 FB이라는 state로 가고요.
FB state에 있었으면 0.9의 확률로 다시 FB을 하거나 혹은 0.1의 확률로 C1으로 다시 돌아가거나, 이와 같은 구조로 되어 있다고 보시면 되겠습니다.
그래서 이와 같은 모델이 주어졌을 때 state transition function은 밑에 주어진 대로 P라고 주어질 수 있겠죠. 그래서 아까 말씀드렸다시피 P는 n×n 행렬로 표현됐다고 말씀을 드렸잖아요.
그래서 이 그림을 보시면 지금 state가 C1부터 C3, Pass, Pub, FB, Sleep 이런 총 7개의 state로 구성되어 있고요.
그래서 그 7개의 state에 대한 state transition function은 7×7 행렬로 주어지게 되고요.
일단 이 state transition function을 구체적으로 보면 내가 현재 C1이라는 state에 있다고 하면 C2로 transition할 확률이 0.5이고요. FB으로 transition할 확률이 0.5입니다.
여기 빈칸으로 되어 있는 부분은 그냥 다 0이라고 보시면 대요. 그래서 C1에서 C1로 가는 확률은 0이다. 이런 식으로 볼 수 있고요.
C2도 마찬가지로 내가 C2에 있었을 때 C3로 갈 확률이 0.8이고요. Sleep이라는 state로 갈 확률은 0.2이다.
여기서 재미있는 특징 중의 하나는 각 행에 대해서 합을 구해보면 다 1이라는 걸 알 수 있습니다.
왜냐하면 내가 한 state에서 모든 state에 대해서 어떻게 갈지에 대한 확률분포의 합은 1이 되어야겠죠.
여기 위에 보시면 에피소드라는 얘기가 있습니다.
에피소드는 뭐냐면 제가 이니셜 state로부터 시작해서 주어진 state transition 함수를 이용해서 하나의 경험을 샘플링 하는 거다. 이렇게 보시면 됩니다.
그러니까 제가 C1부터 시작해서 0.5의 확률로 FB, 혹은 C2로 가는데 제가 동전 던지기 같은 걸 해서 C2로 갔고요. 그다음에 C3로 갔고 Pass하고 Sleep으로 가겠죠.
그래서 결국에는 시작 state는 C1이고, 종료 state는 항상 Sleep입니다. Sleep인데 제가 주어진 P라는 확률분포에 따라서 random 하게 한번 가보는 것이죠.
가봐서 그 하나하나를 에피소드라고 합니다. 그러니까 쉽게 얘기하면 여러분한테 P가 주어진다고 하면 여러분이 예를 들어서 100개의 에피소드를 샘플링할 수 있는 겁니다.
그러니까 주어진 확률분포대로 이렇게 가보고 저렇게 가보고 해서 터미널 로드까지 갈 때 그 전 과정 하나를 에피소드라고 하는 것이죠.
그래서 실제로 강화 학습에서 게임을 하는 인공지능을 만든다고 할 때도 실제로 게임을 여러 판을 하게 되죠. 그 여러 판이라는 게 결국 하나하나가 에피소드가 될 거고요.
그 에피소드로부터 P를 예측할 수도 있고 이런 policy도 구할 수 있고 그런 식으로 된다고 보시면 되겠습니다. 그래서 P가 주어졌을 때 에피소드를 샘플링할 수도 있고요.
혹은 여러분의 에피소드를 아주 많이 얻은 다음에, 시뮬레이션을 통해 얻은 다음에 그것으로부터 P를 예측할 수도 있습니다.
그러면 지금까지 Markov process에 대해서 얘기했고요. 그거보다 한 단계 더 나간 Markov reward process라는 것에 대해서 말씀을 드리도록 하겠습니다.
여기서 보시면 빨간색으로 표시된 컨퍼런트들이 있는데 빨간색으로 표시된 건 지금까지 말씀드린 Markov process에서 추가된 컨퍼런트를 보여드리고 있습니다.
그래서 state S이랑 transition function은 Markov process와 동일하고요. 그 외에 추가적으로 reward function과 discount Factor라는 게 주어집니다.
discount Factor는 하나의 스케일러 값이고 이거는 0부터 1 사이의 값을 가집니다. 그리고 reward 함수는 뭐냐면 정의역은 state이고 공역, 치역은 실수입니다.
그러니까 어떤 state가 주어졌을 때 그 state의 어떤 reward 값을 맵핑해주는 함수라고 보시면 되겠습니다.
그래서 구체적인 정의가 오른쪽에 나타나 있는데, 내가 S라는 state에 있을 때 어떤 reward를 받을 기댓값을 이런 식으로 표현할 수 있습니다.
그래서 좀 전에 Student MRP, 그러니까 Markov process Example을 보여드렸는데 그거를 MRP Example로 확장한 것입니다. 그러니까 이거 하나는 MRP의 예제라고 보시면 되겠고요.
그래서 여기서 보셨다시피 우리가 새로 추가되어야 할 거는 R과 γ입니다.
그래서 R이라는 건 state 각각에 대해서 어떤 실수 값을 주어야 하기 때문에 이렇게 빨간색으로 보셨다시피 각 state마다 reward 값을 assign을 한 것이죠.
그리고 엄밀하게 여기 discount Factor에 대해서 주어지지는 않았지만 예를 들어서 discount Factor γ를 1로 세팅했다.
그런데 이 Example 자체가 하나의 MRP Example이라고 이해하시면 되겠습니다.
이제 Markov process에서는 state와 state transition밖에 없었는데 여기 Markov reward process에서는 reward라는 새로운 개념이 추가가 됐죠.
그렇기 때문에 우리가 return이라는 개념과 value function이라는 개념을 말씀드릴 수 있습니다.
그래서 return이라는 건 뭐냐면 total discount reward from time step t입니다. 그래서 현재 time step t로부터 해서 제가 미래에 얻을 수 있는 reward를 다 합한 것입니다.
중요한 거는 discount가 된다는 것이죠. 그러니까 한 step 미래의 reward에 대해서는 γ만큼, 두 step 후의 reward에 대해서는 γ²만큼,
그러니까 그만큼 discount가 된 reward를 정의하고 그 모든 걸 다 더한 값이 return이라고 보시면 되겠습니다.
그래서 immediate reward, 그러니까 당장 얻을 수 있는 reward가 나중에 delayed reward보다는 훨씬 더 높은 가치를 줘야 한다는 거고요.
그래서 여러분이 만약에 γ를 0으로 줬다는 건 여러분이 근시안적인 모델을 만들었다는 거죠. 이렇게 학습을 하다 보면 이 에이전트는 지금 당장의 reward만 따라가도록 학습이 될 거고요.
만약에 γ를 1로 줬다. 이거는 지금 당장의 reward랑 아주 먼 미래의 reward랑 똑같이 고려하겠다는 거기 때문에 훨씬 멀리 보는 에이전트를 학습할 수 있게 되겠죠.
그래서 이 γ에 대한 얘기는 다음 슬라이드에서 다시 한 번 말씀드리도록 하겠습니다.
그다음에 reward라는 개념이 새로 들어왔기 때문에 이제 value function을 정의할 수 있습니다. value function은 어떻게 정의하느냐면 현재 state의 value는 어떻게 정의하느냐?
내가 만약에 t라는 step에 s라는 state에 있을 때 내가 return을 Gt라고 했을 때 얻을 수 있는 return의 expectation 값이다. 이렇게 엄밀하게 정의하게 됩니다.
그래서 여기 보시다시피 expected return starting from state s 이런 식으로 정의할 수 있습니다.
그러면 discount Factor에 대해서 자세히 말씀드리도록 하겠습니다. 여러분이 ‘이게 왜 필요하냐?’ 이렇게 생각할 수 있는데요. 한 가지 예제와 함께 왜 discount가 필요한지 말씀드리도록 하겠습니다.
지금 여기 하나의 Markov reward process의 예제가 있습니다. 여기서 중요한 점은 뭐냐면 state1과 state2를 보면 자기 자신으로 돌아오면서 reward를 1로 받는 그런 edge가 존재한다는 것이죠.
그래서 만약에 여러분이 S1, 1이라는 state에 있을 때의 value를 구해보면 이게 무한대가 되는 문제가 있습니다.
왜냐하면 내가 S1에 있을 때 A를 취하고, A를 취하고, A를 취하면 계속 state1에 머무르면서 계속 reward를 취할 수 있겠죠.
그래서 discount Factor가 없다면 이런 infinity한, 그러니까 value function의 값이 무한대가 되는 이런 안 좋은 상황이 벌어지게 됩니다.
여기 Q라는 value function이 나와 있는데요. 현재로는 Q라는 value function이 뭔지에 대해서는 이해하실 필요가 없고요.
이게 나중에 배울 value function을 구하는 equation인데, 우리가 나중에 배울 value function equation을 적용해보면 S1에서의 value가 무한대가 된다.
일단 현재로서는 그렇게만 이해하시면 되겠습니다.
그래서 만약에 여러분의 모델에서 cycle edge가 존재하고요. 이 cycle edge의 reward가 0이 아니라고 하면 무한대가 되는 안 좋은 현상이 일어납니다.
그래서 만약에 discount Factor가 없는 경우에 이런 상황이 벌어진다는 것이고요.
그런데 만약에 discount Factor가 있다. 그렇다고 하면 나중에 설명을 하겠지만 S1의 value function 값이 1+γ, Q가 되고
이거는 다시 1+γ1+γ²Q, 그리고 이거는 다시 1+γ+γ²+γ³Q 이런 식으로 표현이 됩니다.
그래서 γ가 만약에 1보다 작은 수가 된다면 이게 무한한 텀을 가진 summation이라고 하더라도 특정 값에 bound가 되죠. 그러니까 극한 값을 취하면 특정 값으로 수렴하게 됩니다.
그래서 이런 무한대가 되는 걸 막을 수가 있다는 장점이 있고요. 그리고 이게 상식적, 물리적으로 생각해도 discount Factor는 매우 타당합니다.
여러 가지 방식으로 해석할 수 있겠죠. 그러니까 수학적으로도 γ가 있으면 안정화되는 효과뿐만 아니라 여러 가지로 해석할 수 있는데
이거는 한마디로 보면 시간이 지나면 지날수록 에이전트가 살아있을 확률이 점점 줄어들 텐데 그거에 대한 표현일 수도 있고요.
그다음에 내가 현재 봤을 때 내가 두 step 후에 얼마만큼의 reward를 받을 수 있을 거라고 생각하지만 세상에는 불확실성이 존재하고 그만큼 discount를 해줘야 한다. 그렇게 말씀드릴 수 있고요.
또 하나는 당연하게도 당장의 reward는 아주 먼 미래의 reward보다는 더 가치 있게 평가를 받아야 하고요.
그리고 마지막으로 수학적으로도 γ가 있느냐, 없느냐에 따라서 무한대가 발생할 수 있느냐, 없느냐? 이런 여러 가지 경우가 가능하다는 것입니다.
그래서 이거는 MRP에 대한 예제를 value function을 어떻게 구하는지 예제를 들어본 것입니다.
그래서 여기서는 저희가 starting 포인트로 C1. starting state는 항상 C1이고, discount Factor는 0.5입니다.
그래서 value function을 어떻게 구하는지의 하나의 예를 보여준 건데 아까도 말씀드렸다시피 우리가 random process가 하나 주어지면 여러 개의 에피소드를 샘플링할 수 있습니다.
예를 들어서 우리가 100개의 에피소드를 샘플링했다고 해보죠. 그러면 첫 번째 걸 보면 C1에 있을 때 reward가 -2였죠. 그다음에 C2일 때 reward가 -2입니다.
-2인데 discount를 해줘야 해서 2분의 1을 곱해야 되겠죠. 그다음에 C3로 갔으면 다시 마이너스입니다. 그런데 γ가 한 번 더 곱해지니까 4분의 1이 되죠.
그런 식으로 하다 보면 첫 번째 에피소드에 대해서 C1이라는 state에 있었을 때의 값은 -2.25가 됩니다.
그래서 이런 간단한 산술을 여러분이 모든 에피소드에 대해서 다 구할 수 있고요. 아까도 말씀드렸다시피 G1,
그러니까 state1에서의 return이라는 건 다음 step의 reward로부터 해서 계속 γ만큼 discount하면서 끝까지 갔을 때의 reward의 총 합이라고 말씀드렸고,
value function은 그거에 대한 expectation이라고 말씀드렸죠. 그래서 이런 에피소드를 만약에 여러분이 100개를 샘플링했다고 하고
그 에피소드 100개에 대해서 V1값을 다 구한 다음에 걔들의 평균을 내주면 V of C1, 그러니까 C1일 때의 value function값을 구할 수 있습니다.
그래서 오늘은 Markov process와 Markov reward process의 개념과 예제를 말씀드렸고요.
그다음에 Markov reward process에서 중요한 정의인 return, G라고 표현을 하고요. 그다음에 value function, V를 소개시켜드렸습니다.
그러면 다음 시간에는 우리가 정작 배워야 할 Markov Decision Processes와 거기서부터 어떻게 Prediction 혹은 Control을 수행하는지에 대해서 간단히 배워보도록 하겠습니다.
그러면 다음 시간에 다시 뵙도록 하겠습니다.
지난 강연에서는 저희가 Bellman Expectation Equation에 대해서 소개를 드렸고요. Bellman Expectation Equation이라는 건 결국에 V 혹은 Q라는 value function을 구하는 과정이었습니다.
그래서 V가 어떻게 표현이 되는지, Q가 어떻게 표현이 되는지를 말씀드렸고요. 그리고 V나 Q를 구하기 위해서 행렬의 역행렬을 곱하는 방법에 대해서 말씀드렸습니다.
그때 제가 그걸 좀 더 효율적으로 구할 수 있는 방법이 있고, 그때 Dynamic programming, 그러니까 동적 프로그래밍을 이용하는 방법이 있다고 말씀을 드렸는데요.
본 강의에서는 어떻게 그 value function을 동적 프로그램을 통해서 구하는지에 대해서 설명을 드리도록 하겠습니다.
Dynamic programming이라는 건 여러분이 만약에 알고리즘 수업을 들었다면 반드시 꼭 배워야 되는 부분 중에 하나고요.
본 강연에서는 여러분이 Dynamic programming에 대한 사전지식이 있다고 가정하고 말씀을 드리도록 하겠습니다.
Dynamic programming은 단순히 지금 말씀드리는 강화학습에서만 쓰이는 건 아니고 되게 다양한 분야에서 해를 구하는 알고리즘으로 사용이 되고요.
기본적으로 다음 두 가지 특성을 만족하는 경우에 Dynamic programming을 사용할 수 있습니다.
그러니까 주어진 문제가 있다고 하면 그 주어진 문제를 여러 개의 sub-problem, 더 작은 문제로 나눌 수가 있고요.
그리고 중요한 건 그 나눠진 작은 문제에 대해서 저희가 솔루션을 구하면, 그 솔루션이라는 게 원래 복잡한 문제를 푸는 솔루션을 구하는 데 사용이 된다는 것입니다.
그러니까 원래 문제를 계속 작은 문제로 나눠가고요. 그래서 충분히 작아진 문제에 대해서 솔루션을 구해서
그 솔루션을 활용해 그것보다 더 높은, 더 어려운 문제에 대한 솔루션을 구하고 구하는 이런 과정이 가능한 경우에 Dynamic programming을 적용하게 됩니다.
말씀드린 MDP에서는 이 두 가지 특성을 다 만족하기 때문에 Dynamic programming을 활용해서 Bellman Expectation Equation을 풀 수가 있다고 말씀을 드리겠습니다.
그리고 이건 예전에 말씀드린 것에 대한 리마인드입니다. 다시 한 번 말씀드리고자 하는 내용인데요.
Prediction이랑 Control에 대한 정의를 그때 말씀드리면서 강화학습에서 Prediction이라는 건 policy가 주어졌을 때 value function 구하는 거라고 말씀을 드렸고요.
그래서 좀 더 구체적으로 지금 MDP setting에서 말씀을 드리면, MDP가 주어지고 policy가 주어졌을 때 value function을 구하는 과정이 Prediction이 됩니다.
Control은 뭐냐? Control은 optimal policy를 구하는 과정이라고 했잖아요. 그래서 마찬가지로 이 MDP setting에서는 MDP가 주어졌을 때 optimal policy를 구하는 과정이라고 보시면 되겠습니다.
그런데 전 수업 때 잠깐 말씀드렸다시피 optimal policy이라는 건 optimal value function을 구하면 되게 쉽게 구할 수가 있습니다.
그래서 많은 경우에 Control 문제라고 하면 optimal policy를 구하는 문제이긴 한데, 그냥 optimal value function을 구하는 문제를 Control이라고 부르기도 합니다.
그리고 여기서 MDP가 주어졌다는 얘기가 있는데 그건 무슨 얘기냐 하면, S도 알고, A도 알고, P, R도 알고, γ도 알고 있다. 이게 다 주어졌다고 가정을 한 상태입니다.
그래서 이 5개의 component가 다 주어졌을 때 policy가 추가적으로 주어지면 value function을 구하는 문제. 그게 Prediction이고요.
MDP만 주어졌을 때 optimal policy를 구하는 과정이 Control이다.
그래서 Iterative Policy Evaluation을 통해서 Prediction을 해결한다. 이걸 다음 슬라이드부터 말씀을 드리도록 하겠습니다.
Iterative Policy Evaluation이라는 건 뭐냐 하면, policy가 주어졌을 때 이걸 evaluation 한다. 그러니까 전 슬라이드에서 말씀드렸다시피 Vπ를 구하는 과정입니다.
Vπ를 어떻게 구하느냐 하면, 다음과 같이 구합니다.
그래서 처음에 여러분이 V를 initialize한 다음에 그걸 계속 matrix multiplication으로 주어진 수식대로 반복을 하면, 결국에 V가 수렴이 되는데요.
그 V가 수렴이 된 결과를 policy에 대한 evaluation이 끝났다고 보실 수가 있습니다.
그러니까 예를 들어서 만약에 state가 10개다. 그러면 V를 1로만 이루어진 10×1 벡터로 정의를 합니다.
그리고 MDP가 주어졌다고 얘기했잖아요. MDP가 주어졌다는 얘기는 R이 주어졌고, P도 주어졌고, γ도 주어졌습니다.
그러니까 1, 1, 1, 1부터 시작해서 matrix multiplication을 하고, 그 각각의 요소에 γ를 곱하고, 그걸 Reward랑 더하면 다음 Iteration의 V값을 구하죠.
그 V값을 활용해서 우항에 다시 넣는 것입니다. 그래서 다시 γ 곱하고, P 곱하고, R 더하고.
이 과정을 계속 반복해서 이게 결국이 수렴이 되는데, 그 수렴된 값이 policy가 주어지고 MDP가 주어졌을 때 value function 값이라고 보시면 되겠고요.
이거는 일종의 리마인드인데, Bellman Expectation Equation을 아래와 같이 처음에 정의했고 그거에 대한 matrix 형태는 위와 같다고 말씀드렸는데요. 그거에 대해 다시 한 번 보여드리고 있습니다.
그래서 지금 말씀드린 과정에 대해서 한번 예제와 함께 보도록 하겠습니다. 지금 Problem setting은 어떻게 되어 있느냐 하면, 여러분이 취할 수 있는 action은 이와 같고요.
위로 가든지, 오른쪽으로 가든지, 왼쪽으로 가든지, 아래쪽으로 가든지.
그리고 일단 계산을 쉽게 하기 위해서 γ는 1로 주어졌다고 가정을 해보고요. 그리고 Terminal state, 종료되는 state는 왼쪽, 오른쪽의 회색. square가 Terminal state고요.
그리고 만약에 여러분이 1이라는 state에 있을 때 ‘북쪽으로 가라’라는 action을 취하면 아무데도 안 가고 그냥 1에 머물러 있습니다. 그래서 세 번째 얘기가 그 말씀을 드리는 거고요.
그리고 Reward는 어떻게 정의하느냐 하면, 갈 때마다 -1, move를 한 번 할 때마다 -1로 주어졌습니다.
그러니까 이 Reward로 주어졌다고 하면 최단거리로 이 Terminal state로 가는 policy가 학습이 되겠죠.
그리고 마지막으로 policy를 정해줘야 하죠. 지금까지는 MDP를 정의했고, 마지막으로 policy를 정의해줘야지만 그때의 value function을 구할 수 있는데요.
우리가 여기서는 Random policy를 가정했습니다. 그러니까 모든 state, 어떤 state든지 간에 북으로 가거나, 동으로 가거나, 남쪽으로 가거나, 서쪽으로 가는 확률이 다 0.25로 동일하다.
이런 문제 세팅이 주어졌다고 한번 가정을 해보고요. 이때의 value function을 구하는 것입니다. 이게 우리의 목적이고요.
value function을 구한다는 건 이 수많은 1~14의 state들이 있는데, 거기에서의 value값을 구하는 것입니다.
다음 슬라이드에 말씀드릴 Iterative Policy Estimation을 통해서 value값을 구하는 과정입니다.
그래서 소개시켜드린 대로 다음 step의 value function이라는 건 Reward+γ+state transition probability P+V. 이런 식으로 표현이 됐죠.
처음에 Iteration 0에서는 V를 모두 다 0으로 initialize합니다. 그런 다음에 P를 곱하고 γ를 곱한 다음에 다시 R을 더하면 이와 주어지고요.
이렇게 새롭게 구한 V를 가지고 다시 equation에 넣어서 P를 곱하고 γ를 곱하고 다시 R을 더하면 그다음 Iteration 2에서의 value function이 나옵니다.
그래서 이 과정을 계속 반복하다가 수렴될 때까지 진행을 하다 보면, 결국에 마지막으로 얻는 이 value function 값이 저희가 구하고자 하는 Vπ(s).
그러니까 Random policy를 우리가 따라간다고 했을 때 각 state의 value function 값을 이와 같이 구할 수가 있습니다.
그래서 좀 전에 말씀드린 내용은 evaluation, 그러니까 V를 구하는 과정에 대해서 말씀을 드렸고요.
여기 그림을 보시면 오른쪽에 Greedy policy에 대해서 소개가 되어 있는 것이 있습니다. 그런데 현재까지 구해진 value function을 기반으로 해서 Greedy policy를 구한 것입니다.
그래서 Iteration 0일 때를 보시면, 이 state를 보시면 오른쪽도 0.0, 아래도 0.0, 왼쪽도 0.0이기 때문에 이 state에서는 어디를 가든지 간에 다 동일합니다. 그래서 모든 방향으로 화살표가 있고요.
그리고 다음 step에서 V를 새롭게 구하면 이런 state에서는 마찬가지로 위, 아래, 왼쪽, 오른쪽이 다 -1이기 때문에 다 똑같은 확률로 구해지지만,
이 state를 기준으로 본다면 왼쪽은 0이고, 오른쪽, 아래쪽은 -1이기 때문에 여기서는 왼쪽으로 가는 optimal policy로 업데이트가 되죠.
저희가 계속 V를 구해가면서 그때마다 각 state에서 value function이 최대가 되는 방향으로 화살표를 긋는다면, Greedy policy를 계속 구할 수가 있고요.
그런데 재미있는 사실은 뭐냐 하면, value function을 구하려면 Iteration을 되게 많이 돌렸어야 되는데,
Greedy policy 같은 경우에는 V3, 그러니까 step3까지만 V를 구하면 optimal policy, 그러니까 수렴된 Greedy policy를 구할 수 있다는 사실입니다.
그래서 여기서 재미있는 건 뭐냐 하면, 여러분이 만약 policy만 구하고 싶은 경우에는 더 적은 Iteration만으로도 구해낼 수 있는 경우가 꽤 있다.
그리고 만약에 evaluation하고 싶다고 하면 충분히 Iteration을 돌려서 V가 더 이상 바뀌지 않을 때까지 수행을 해야 된다. 이 예제와 함께 볼 수 있었습니다.
그러면 여기서 또 하나 재미있는 내용은 Policy Improvement에 대한 내용입니다.
그러니까 좀 전에 말씀드렸다시피 policy가 주어졌을 때 evaluation하는, 그러니까 Vπ를 구하는 과정은 전 슬라이드에서 했습니다.
그런데 만약에 π가 주어졌을 때, policy가 주어졌을 때 Vπ를 구한 다음에 그 구해진 Vπ를 가지고 혹시라도 policy를 Improve할 수 있지 않을까? 그거에 대한 질문입니다.
그러니까 그 전 예제를 보면 우리가 처음에 가정한 policy라는 건 Random policy였습니다. 그러니까 어느 state든지 간에 Random으로 움직여라.
그런데 그 전 example에서 보셨다시피 오른쪽 그림에 보여드린 내용은 우리가 V를 계속 구함에 따라서 optimal policy도 계속 업데이트 된다는 걸 아실 수 있겠죠.
그래서 어떻게 하면 우리가 안 좋은 policy로부터 시작해서 더 나은 policy를 구할 수 있을까? 그 과정이 전 예제에서 힌트로 주어져 있다고 보시면 되겠습니다.
실제로는 어떻게 하느냐? policy를 계속 개선하려면 어떻게 하느냐? 이와 같이 하시면 됩니다.
여러분이 일단 policy를 하나 정합니다. 그러니까 그 전 슬라이드처럼 여러분이 특별한 아이디어가 없으면 그냥 Random policy처럼.
‘각 state에서 Random으로 어떤 action을 취해라.’ 이런 식으로 policy를 정합니다. 그렇게 정한 다음에 여러분이 그 policy에 대한 value function을 구합니다.
그다음에 구해진 value function에 대해서 policy를 Improve 하는데, policy를 개선하는데 어떻게 하느냐 하면, Greedy 아이디어를 통해서 policy를 한번 업데이트 해봅니다.
그러니까 Greedy라는 건 뭐냐 하면, 각 state에서 주변에 내가 취할 수 있는 action을 쭉 봐서 value function이 최대가 되는 action을 취하도록 그런 식으로 policy를 바꾸는 겁니다.
그렇게 바꾼 다음에 그러면 새로운 policy가 생겨났죠? 그럼 그 새로운 policy를 가지고 다시 Vπ를 구합니다. 그래서 value function을 다시 구합니다.
그래서 그 value function에 대고 Greedy하게 action을 취해나가면 또 새로운 policy가 나오겠죠. 이 과정을 계속 반복하다 보면 결국에 optimal policy로 수렴이 된다. 그게 밝혀진 사실입니다.
그래서 그걸 도식화하면 다음과 같습니다. 여러분이 처음에 policy를 initialization을 해요. 그리고 마찬가지로 value function도 initialize를 합니다.
value function은 좀 전에 예제에서는 다 0으로 initialize 했죠. 그다음에 Iterative Policy Evaluation을 통하면 새로운 value function을 구할 수 있었죠.
그 value function에 대고 Greedy한 action을 취하는 policy로 새롭게 업데이트를 합니다.
그래서 그 policy가 업데이트 되면, 그 policy를 기반으로 다시 Iterative Policy Evaluation을 통해서 V값을 구하고요.
이 과정을 계속 반복하다 보면, 여러분이 V*, 그러니까 optimal한 value function과 optimal한 policy를 구할 수 있다. 이게 밝혀진 사실입니다.
그래서 Policy Iteration이라는 방법은 여러분이 optimal policy를 구하는 과정 중에 하나고요. 좀 시간이 걸리고 Iterative한 방법이긴 하지만, V랑 π를 initialize해서 일단 V 구하고,
그 V 구한 걸 기반으로 Policy Improvement 하고, Improve된 policy로부터 value function을 다시 구하고. 이 과정을 반복하면 결국에 수렴이 된다.
그래서 MDP에서는 이와 같이 여러분이 원하는 value function도 언제든지 구할 수 있고요. 원하는 policy도 언제든지 구할 수가 있습니다.
그래서 지금까지 말씀드린 내용에 대해서 요약을 하자면 다음과 같습니다. 크게 보면 문제가 두 개 있었죠? Prediction이 있었고 Control이 있었습니다.
Prediction은 policy가 주어졌을 때 그리고 MDP가 주어졌을 때 value function을 구하는 것이고요. Control이라는 건 MDP가 주어졌을 때 optimal policy를 구하는 것입니다.
그래서 좀 전에 말씀드린 내용으로 보시면 Bellman Expectation을 활용하고 그다음에 Iterative Policy Evaluation이라는 방법을 활용하면 여러분이 Prediction을 풀 수 있었습니다.
그리고 Control은 어떻게 푸느냐? Iterative Policy Evaluation 해서 value function이 구해지면 Greedy Policy Improvement를 하고요.
Greedy Policy Improvement를 하면 그거에 대해서 다시 Iterative Policy Evaluation을 하고요.
이 두 과정을 반복하는 것이고, 이 두 과정을 반복하는 방법을 Policy Iteration이라고 한다. 이렇게 보시면 되겠습니다.
지금까지 설명 드린 내용을 보시면, 제가 V를 활용해서 V 구하는 방법과 그다음에 그 구해진 V로부터 optimal policy 구하는 과정, 이런 과정을 설명 드렸는데요.
그런데 여러분이 똑같은 과정을 Q에 대해서도 할 수가 있습니다.
Q를 initialization 한 다음에 그 Q를 가지고 Iterative Policy Evaluation으로 주어진 policy에 대한 Q를 구하고, 그 Q에 대해서 다시 Greedy Policy Improvement를 하는 식으로 해도 됩니다.
그런데 좀 전에 보여드린 대로 V를 주로 다루는 이유는 뭐냐 하면, V를 다룰 때 Complexity가 더 적기 때문입니다.
왜냐하면, action이 총 m개가 있고 state가 총 n가 개가 있으면, 만약 V를 활용하면 복잡도가 m×n제곱입니다.
그런데 여러분이 Q를 활용하면 m제곱×n제곱입니다. 그러니까 V를 활용하는 게 더 효율적으로 구할 수 있다는 거고요.
상식적으로 생각하면 Q는 각 s에 대해서, 각 a에 대해서 다 정의를 해줘야 되죠.
V값은 어떻게 보면 state가 n개라고 하면 n×1 벡터로 표현할 수가 있지만, Q는 어떻게 됩니까? state가 n개고, action이 m개라고 하면 n×m matrix로 표현이 되죠.
그러니까 Q를 다룬다는 건 더 많은 메모리가 필요하고, 더 많은 computation이 필요하다고 이해하시면 되겠습니다.
지금까지는 MDP가 주어졌을 때 Dynamic programming 아이디어를 활용해서 저희의 주요한 두 가지 문제,
그러니까 value function 구하는 문제, policy 구하는 문제를 어떻게 할 수 있는지에 대해서 소개를 시켜드렸고요.
물론 이거 외에 Prediction을 하고 Control 하는 방법이 되게 다양하게 많이 존재하는데, 안타깝게도 본 강의에서는 일단 여기서 마치는 걸로 하고요.
혹시라도 강화학습에 더 관심이 많으시다면, 강화학습에 더 초점에 맞춰진 수업이나 책 그리고 강연을 참조하시기 부탁드립니다. 그럼 오늘의 강연은 여기서 마치도록 하겠습니다.
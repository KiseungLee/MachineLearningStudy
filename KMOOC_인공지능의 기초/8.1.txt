안녕하세요? 서울대학교 컴퓨터공학부의 김건희입니다. 오늘은 인공지능의 기초 마지막 시간이고요. 오늘의 주제는 Computer Vision입니다.
사실 처음에도 말씀드렸다시피 인공지능은 매우 큰 분야이기 때문에 8개의 강좌로 충분히 다 말씀드리기에는 시간적인 한계가 있긴 합니다.
그래도 저 개인적인 입장에서는 중요하다고 생각되는 주제들 위주로 말씀드렸고요.
각각이 말씀드린 대로 하나의 매우 큰 분야이기 때문에 이 강좌를 통해서 여러분이 인공지능을 단순히 일반 사람들이 이해하듯 막연하게 생각하지 말고
‘인공지능이 이러이러한 분야가 있고, 이런 것들의 소개 강좌를 들어봤더니 더 관심이 생기더라. 그래서 더 알아보고 싶다.’ 이런 기회가 됐으면 합니다.
마지막 차수의 주제는 Computer Vision입니다. Computer Vision은 인공지능 중에서 시각 지능에 대한 내용입니다.
그러니까 사람은 눈으로 사물을 이해하고 그리고 우리가 원하는 곳까지 안전하게 이동할 수가 있는데요.
‘그와 같은 능력을 기계도 한번 가질 수 있도록 해보자’ 이게 Computer Vision의 가장 중요한 목적이라고 할 수 있겠고요.
그리고 저 개인적으로도 인공지능 분야 중에서 Computer Vision에 가장 많은 연구를 하고 있기 때문에 저 개인적으로도 매우 친숙한 분야이기도 합니다.
그럼 Computer Vision에 대해서 소개를 시켜드리도록 하겠습니다.
그러면 첫 번째로 과연 Computer Vision이 무엇인지에 대해서 알아보도록 하겠습니다.
사실 Computer Vision의 정의는 여러 가지가 가능할 텐데요. 제가 개인적으로 가장 만족스러운 정의를 여러분께 소개시켜드리고자 합니다.
여기 보시다시피 Computer Vision이란 결국에 우리가 사람으로서 눈을 가지고 할 수 있는 정말 많은 일이 있는데, 그 일들을 기계 혹은 컴퓨터에게 가르쳐주는 학문이라고 보시면 되겠습니다.
우리가 시각지능을 갖추어서 여러 가지 일을 하는데, 그 시각지능의 수많은 작업들을 기계도 역시 할 수 있도록 가르치는 것이 Computer Vision의 목적 그리고 그 정의라고 이해하시면 되겠습니다.
Computer Vision이란 학문 자체가 사람의 시각 능력을 모사하기 위해서 시작됐다고 할 수 있기 때문에 과연 사람의 비전, 사람의 시각 능력이 어떤지 한번 살펴보도록 하죠.
여기 몇 가지 리스트 업을 했다시피 우리는 시각 능력을 가짐으로써 정말 많은 그리고 놀라운 일들을 할 수 있습니다.
우리가 주변의 사람도 알아볼 수가 있고요. 그리고 여러 가지 물체도 인식할 수 있고요. 또 그 물체들 중에 내 거, 남의 거 이런 식으로 속성까지도 다 알 수 있습니다.
또 하나 중요한 특징 중에 하나는 제가 걸어서 강의실까지 오는 것도 제 눈이 있어서 가능한 일이었고요.
그래서 여러분이 시각 능력을 갖춤으로써 주변 환경에 수많은 장애물들이 있지만, 거기에 부딪히지 않고 안전하게 여러분이 원하는 곳으로 이동할 수도 있습니다.
그리고 세 번째 내용은 어떻게 보면 말로 설명하기 힘든 우리의 놀라운 능력 중에 하나라고 할 수 있는데요.
우리는 주변 환경을 보고 분위기도 이해할 수 있습니다. 그러니까 지금 내가 어떤 말을 하고 싶을 때 지금 내가 말을 해도 되는 분위기인지 아닌지도 시각 능력을 통해서 할 수가 있습니다.
그리고 또 하나 놀라운 능력은 여러분에게 사진 한 장만 준다고 하더라도 여러분은 그 사진으로부터 정말 긴 이야기를 만들어낼 수 있습니다.
이 모든 것이 다 우리가 시각 능력을 갖추어서 사진을 보고 사진을 다 이해할 수 있기 때문에 가능합니다.
하지만 사람의 시각 능력은 완벽하다고 할 수 없습니다. 그래서 몇 가지 단점에 대해서 말씀드리면, 첫 번째 단점은 우리는 착시현상을 겪는다는 것입니다.
그거에 대한 몇 가지 예는 다음 슬라이드에서 말씀드리도록 하겠습니다.
그리고 사람은 어떤 사진을 볼 때 전체에서 가장 중요한 내용을 빠르게 알아내는 능력은 있지만, 세부 디테일을 자세히 보거나 혹은 그걸 기억하는 능력은 매우 떨어집니다.
그리고 우리는 주변 상황을 어떻게 보면 내 식대로 보고, 내 식대로 이해하고, 내 식대로 설명을 한다고 볼 수 있고요.
그래서 여러분이 수많은 분들에게 사진 한 장을 주고 그것에 대해서 한번 기술해보라고 요청을 한다면, 아마 100명이 다 다른 문장을 써낼 것입니다.
그리고 또 하나의 단점이라고 할 수 있는 건 우리는 주변 환경을 잘 이해하긴 하지만, 주변 환경을 정확하게 이해하지는 못합니다.
그러니까 제가 어느 방에 들어가서 그 방의 사이즈가 과연 천장이 1.5m인지 1.6m인지 이런 건 전혀 알 수가 없고요. 그리고 어떤 물건이 주어져도 그거의 정확한 크기, 길이는 잘 알기가 어렵습니다.
그리고 또 하나는 우리의 기억력이라는 게 매우 제한이 돼 있어서 아마 여러분에게 한 10장의 사진을 보여드린다고 하면,
첫 번째 사진이 무엇이었는지를 아주 자세히 기억하는 건 매우 어려운 일이 될 것입니다.
그래서 지금 말씀드린 Human Vision의 단점이라고 보여드린 내용에 대해서는 오히려 Computer Vision이 훨씬 더 잘하는 분야입니다.
Computer Vision은 착시현상도 겪지 않고요. 그리고 Computer Vision은 정확하게 환경을 측정할 수도 있는 능력을 가지고 있습니다.
한편으로 보면 인공지능의 좋은 메시지라고 할 수 있는데요.
인공지능이 단순히 사람을 대체하기 위해서 개발된다기보다는 기계가 잘할 수 있는 분야와 사람이 잘할 수 있는 분야는 많은 경우에 다릅니다.
기계와 사람이 결국에는 공존하는 그런 사회가 인공지능의 목적이라고 이해하실 수 있겠습니다.
그럼 Illusion의 몇 가지 예에 대해서 한번 보여드리도록 하겠습니다.
여러분, 왼쪽의 파란색 정육면체를 계속 보고 있으면 두 가지 정육면체를 보게 됩니다.
첫 번째 정육면체는 지금 보여드린 이 면이 앞으로 보이는 정육면체이고, 또 하나의 예는 이 두 번째 면이 앞면으로 보이는 정육면체입니다.
그래서 이 그림을 계속 유심히 보고 있으면 처음에는 한쪽 면을 정면으로 보고 있다가 어느 순간에 다른 쪽 면을 정면으로 보는 직육면체로 이해가 되고요.
그게 갑자기 왔다 갔다 한다. 그런 착시현상을 여기서 보여주고 있습니다.
또 다른 착시현상은 매우 잘 알려진 착시현상입니다. 여기 보시면 체커 보드가 있고요. 그 위에 원 기둥이 있고요. 그리고 빛 반대쪽으로 그림자가 있습니다.
그래서 여러분이 A라는 영역과 B라는 영역을 보시면, B라는 영역이 훨씬 밝아 보일 것입니다.
하지만 우리가 A, B를 연결하는 회색 바(bar)를 그리게 된다면 A와 B라는 영역이 똑같은 밝기의 영역이라는 걸 여러분이 아실 수 있습니다.
그런데 이런 착시현상이 왜 일어날까요? 사람의 뇌는 차이를 인식하도록 학습되었습니다. 그러니까 어떤 주어진 영역의 밝기가 정확히 어느 정도의 밝기인지를 인식하도록 설계되어 있지 않고요.
이 영역의 밝기가 주변에 비해서 얼마나 밝은지 혹은 얼마나 어두운지를 잘 인식하도록 학습이 되었습니다.
그래서 A 같은 경우에는 주변에 밝은 영역이 있기 때문에 얘가 어두워 보이는 거고, B 같은 경우에 주변이 어두운 영역이기 때문에 더 어둡게 보이는 것입니다.
그러니까 A와 B의 절대적인 밝기는 사람은 잘 특정해내지 못하고 인지해내지 못한다. 이게 이 착시현상에서 보여주는 결론입니다.
Computer Vision에 대해서 본격적으로 말씀을 드리고자 하는데요. Computer Vision이 참 어렵습니다.
왜 어려운지 말씀드리면, 여러분이 왼쪽을 보면 이 사진을 보는 순간 아주 짧은 찰나에 ‘여기 사람 눈, 코, 입이 있고, 사람 얼굴이 있고, 머리가 있고, 어깨도 있구나.
그리고 이 사람은 내가 아는 사람이구나. 아인슈타인이라는 물리학자구나.’ 이걸 정말 보는 순간 알게 됩니다.
그런데 이 사진이라는 게 컴퓨터에는 어떻게 저장이 되어 있느냐 하면, 디지털 이미지 형태로 저장되어 있고요.
디지털 이미지는 해상도라는 게 있습니다. 만약에 이 이미지의 해상도가 600×400 해상도 이미지라고 하면, 그거는 곧 600개의 행과 400개의 열로 이루어진 그리드고요.
그리고 그 각각을 픽셀이라고 부르는데, 그 픽셀은 0~255 사이의 정수 값으로 주어져 있습니다.
그래서 여기서는 gray scale, 흑백 이미지이기 때문에 각 픽셀마다 하나의 숫자가 주어져 있고요.
컬러 이미지 같은 경우에 각 픽셀마다 RGB, 그러니까 빨강, 녹색, 파랑색 3개의 채널에 대해서 각각 0~255 사이의 정수 값으로 구성되어 있다고 보시면 되겠습니다.
사람 입장에서는 이게 너무 당연한 이미지인데, 컴퓨터가 결국에 보는 건 엄청나게 큰 숫자 array일 뿐입니다.
그 숫자 array로부터 ‘이 부분에 눈이 있고, 이 부분에 코가 있고, 이 전체가 사람 얼굴이다.’
이걸 기계한테 가르쳐준다는 게 쉽지만은 않은 일이라는 걸 여러분도 어느 정도 짐작하실 수 있을 것 같습니다.
Computer Vision이 결국에 무엇인지를 요약하는 슬라이드입니다. 그래서 여러분에게 몇 가지 Computer Vision의 정의를 여기에 나열했는데요.
첫 번째 정의는 제가 맨 첫 슬라이드에 소개시켜드린 그 정의입니다. ‘사람으로서 우리가 눈으로 할 수 있는 모든 능력을 기계에게 가르쳐주자.’ 그게 목적이고요.
또 하나의 정의라고 하면 사진을 지능적으로 해석할 수 있는 능력을 Computer Vision이라고 볼 수도 있습니다.
여기서 imagery라고 쓴 이유는 image의 통칭, 여러 종류의 이미지를 다 합쳐서 말씀드리는 것입니다.
우리가 일반적으로 보는 RGB 이미지뿐만 아니라, depth를 측정하는, 거리를 측정하는 이미지도 있을 수 있고요. 여러 형태. 레이저 스캐너로부터 얻어진 이미지들도 있을 수 있고요.
그러니까 그런 모든 이미지 형태에 대해서 지능적으로 이해하는 학문이라고 보실 수가 있고요.
그리고 세 번째 정의는 인공지능이랑 연결돼 있는 정의입니다. 그러니까 대뇌에서 시각 부분을 담당하는 부분을 Visual Cortex라고 합니다.
그러니까 Visual Cortex, 우리 대뇌에서 시각 영역을 담당하는 Visual Cortex을 인공적으로 한번 만들어보자. 그런 학문이라고도 정의할 수 있겠습니다.
그리고 네 번째 정의에서 optics라는 건 광학이고요. 광학이라는 건 빛에 대한 학문을 연구하는 것입니다.
그래서 우리가 실제로 어떤 object, 물체를 보는 것은 조명이 있고, 광원이 존재하고, 그 광원으로부터 빛의 입자가 나가서 물체 표면이 반사가 되고,
그 반사된 빛을 우리 망막이 측정해내기 때문에 빛을 보는 것입니다. 그러니까 이 과정 자체가 광학이라고 할 수 있겠죠.
그런데 Inverse optics라는 건 광학의 반대 방향이라는 겁니다.
그러니까 빛과 물체가 있어서 이미지를 얻는 게 광학이라고 한다면, 우리는 이미지가 주어졌을 때 빛이 어디에 존재하고 환경이 어떻게 구성돼 있는지,
그러니까 반대 방향을 알아내는 과정이라고 보시면 되겠습니다.
여러분이 어떤 정의를 좋아하시든지 간에 Computer Vision은 참 어려운 학문이고요. 대신에 우리가 실제로 보는 것을 다루는 학문이기 때문에 재미는 매우 높다고 할 수 있겠습니다.
그래서 Computer Vision에 대한 여러 가지 작업들에 대해서 말씀을 드리도록 하겠습니다. Computer Vision 작업들은 크게 보면 3가지 레벨로 표현이 가능한데요.
Low-level Computer Vision, Mid-level Computer Vision. 이런 식으로 Computer Vision 연구하시는 분들은 얘기를 합니다.
그래서 Low-level Computer Vision이라는 건 주어진 input도 이미지고 output도 이미지인 경우.
Mid-level 컴퓨터 Vision task라는 건 이미지가 주어졌을 때 output으로 이미지를 얻어내는 게 아니라, 어떤 특정 정보를 나타내는 feature, 특징점을 알아내는 걸 Mid-level 비전이라고 하고요.
High-level은 이미지로부터 어떤 의미를 알아내는 걸 High-level 비전이라고 합니다. 그럼 다음 슬라이드부터 하나씩 예제와 함께 설명해드리도록 하겠습니다.
Low-level 비전의 예입니다. 좀 전에 말씀드렸다시피 Low-level Vision task는 input으로 이미지가 주어졌을 때 output은 또 다른 이미지, output이 됩니다.
첫 번째 예제는 Deblurring입니다. blurring이라는 건 흐릿하다는 뜻이고요. 그러니까 Deblurring이라는 건 흐릿한 부분을 없애서 선명한 이미지로 바꿔준다는 것입니다.
실제 우리가 왜 blurring된 사진을 찍느냐 하면, 저희가 어떤 카메라 셔터를 누르는 순간 우리는 인지하지 못하지만 카메라가 아주 작게 혹은 빠르게 흔들리게 됩니다.
그러면 CCD 카메라의 한 픽셀에 환경의 하나의 포인트가 매핑이 되는 게 아니라 여러 포인트가 CCD 카메라의 한 픽셀에 기록이 되고요.
그러니까 어떻게 보면 각 픽셀마다 환경의 여러 포인트에 대한 평균 정보가 기록됐기 때문에 Deblurring이 발생한다고 이해하시면 되겠고요.
Edge detection에서 Edge라는 것은 이미지를 보시면 어떤 특정 영역은 밝기가 급격히 변하는 부분이 있습니다.
그러니까 이 사진에서는 모자를 따라서 배경과 모자와 경계 부분에 아주 급격하게 밝기가 변하게 되는데, 밝기 변화를 찾아내는 걸 Edge detection이라고 하고요.
그리고 Super-resolution은 뭐냐 하면, 저해상도 이미지를 고해상도 이미지로 바꾸는 것입니다.
여러분이 600×300 이미지를 단순히 확대만 하면 되게 부자연스러운 사진이 나올 것입니다. 좀 더 선명하면서도 자세한 사진을 얻는 것이 Super-resolution이고요.
마지막 예제로 Colorization은 흑백 이미지가 주어지면 그걸 컬러 이미지로 바꿔주는 것입니다.
그래서 이 네 가지 task를 보면 왼쪽이 input이고 오른쪽이 output입니다. 그러니까 이미지가 들어가서 우리가 원하는 더 나은 이미지를 만들어내는 과정이라고 이해하실 수 있겠습니다.
이제 Mid-level Vision task에 대해서 알아보도록 하죠. Mid-level task라는 건 input은 이미지고 output은 단순히 이미지가 아니라 어떤 특징을 가지고 있는 feature가 되겠습니다.
첫 번째 예제는 Boundary detection입니다. Boundary detection이라는 건 경계를 찾아내는 거고요.
좀 전에 살펴본 Edge detection랑 차이를 보시면, Edge는 말 그대로 사진의 밝기가 급격히 변하는 부분이라고 하면, Boundary는 object의 경계라고 할 수 있겠죠.
그래서 도마뱀 표면을 보면 여러 무늬가 있고요. 그 무늬별로 Edge는 detection이 되겠지만, Boundary 같은 경우에 무늬에 대해서는 Boundary가 나오면 안 되겠죠. 그래서 그런 차이가 있고요.
Segmentation은 이미지를 동일한 영역으로 분할해주는 과정을 얘기합니다. 사람 바지 혹은 머리별로 나눠주는 것이고요.
Shape-from-shading은 뭐냐 하면, 사람은 그림자가 보이는 정보로부터 3차원 정보를 알아냅니다.
지금 보여드린 예제를 보시면 오른쪽은 볼록하다고 느끼고 왼쪽은 오목하다고 느낍니다.
그런데 실제로 이건 평면의 사진일 뿐인데 사람은 shading, 그러니까 그림자로부터 shape, 모양을 알아낼 수 있다.
모양 자체가 단순히 이미지가 아니라 그 이미지보다 한 단계 더 나간 feature라고 보실 수 있고요.
또 다른 task는 Alignment입니다. 그러니까 지금 두 이미지에 똑같이 나비가 있지만 나비가 다른 방향에서 찍힌 사진입니다.
어떤 영역이 어떤 영역과 서로 매칭이 되는지를 Align 하는 과정을 Alignment라고 하고요. 그건 단순히 이미지가 output이 아니라, 매칭된 포인트들이 feature로 나오게 됩니다.
그리고 High-level Vision task는 이미지가 들어오면 그거에 대한 의미를 파악하는 것입니다.
첫 번째 보여드린 예제는 Image classification이고요. 첫 번째 이미지가 주어지면 ‘이게 진드기구나.’ 진드기라는 의미로 해석을 해내는 것이죠.
Object detection이라는 건 무슨 물체가 있는지 단순히 이미지를 분류하는 게 아니라, 우선 우리가 관심 있는 물체가 이미지 상 어디에 존재하는지 사각형으로 영역을 지정하고요.
그 영역에 어떤 object가 있는지를 찾아내는 과정입니다. 그러니까 위치까지도 같이 알아야 될 때는 더 어려운 문제죠.
그리고 Image captioning은 근래에 많이 연구되는 분야인데, 이미지가 주어지면 그걸 설명하는 문장을 만드는 것입니다.
Image classification은 이미지를 명사 하나로 변환한다고 하면, Image captioning은 이미지 하나를 문장 하나로 번역을 한다고 보시면 되겠고요.
Pose detection은 우리가 많은 경우에 사진에서 제일 중요한 건 사람입니다. 여러분도 사진 찍을 때 인물 사진을 많이 찍게 되는데요.
주어진 사진에서 사람이 있는 경우에 그 사람이 어떤 자세로 있는지 알아내는 것을 Pose detection이라고 합니다.
지금까지 Low-level부터 High-level 비전까지 여러 예제를 보여드렸는데, 꼭 이거에만 국한되는 게 아니라 아주 다양한 문제가 존재합니다.
그러면 Computer Vision이랑 아주 연관된 분야인 Image Processing, 그러니까 영상처리와의 비교를 말씀드리도록 하겠습니다.
영상처리는 이미지에서 이미지로 변환하는 과정을 주로 다룹니다. 좀 전에 살펴드린 대로 input도 이미지고 output도 이미지이기 때문에 Low-level Vision task와 아주 관련이 깊습니다.
대표적인 작업으로는 이미지 압축, 복원, 개선하는 그런 것들이 있겠고요.
Computer Vision 같은 경우에 Image Processing의 테크닉들을 아주 많이 사용하긴 하지만,
많은 경우에 이미지 분석에 대한 output이 이미지가 아니라 좀 더 그것보다 High-level의 지능적인 분석을 많이 다루게 됩니다. 그런 측면에서 두 학문에 차이가 있다고 보실 수 있겠습니다.
이 그림은 Computer Vision이라는 작업이 아주 Low-level부터 High-level까지 다양한 task로 구성되어 있다고 보실 수가 있고요.
가장 낮은 영역에서는 Image Formation, 그러니까 카메라가 어떻게 동작하는지부터 시작해서 Low-level Vision task.
컬러라든지 혹은 동영상일 경우에 어떤 픽셀이 다음 프레임에서 어디로 움직였는지 그런 모션 정보까지 분석하는 것부터 시작해서 점점 Mid-level task, High-level task까지 있고요.
High-level task가 끝나면 다른 인공지능의 작업들과 연결 관계를 가지게 됩니다.
그래서 아주 낮은 레벨의 광학부터 시작해서 의미론, 다른 인공지능과의 연관관계. 그렇게 아주 다양하게 여러 hierarchy를 가지고 Computer Vision task를 정의할 수 있다고 보실 수 있겠습니다.
그리고 또 하나 중요한 Computer Vision의 특징은 아주 다학제적인 분야라는 것입니다.
여러분이 컴퓨터공학을 전공해도 Computer Vision을 할 수 있지만, 여러분이 Neruo-Science를 전공한다고 하더라도 Human Vision 관점에서 Computer Vision을 해석할 수 있고요.
그리고 여러분이 로봇을 연구한다고 하면 로봇의 많은 경우에 Computer Vision과 함께 문제를 풀어야 되는 경우도 많고요.
그리고 광학을 한다면 물리학도 연관이 되고요. 그리고 요즘 많이들 관심 있어 하시는 기계학습 영역과도 아주 밀접한 관계가 있습니다.
그래서 근래에 Computer Vision 연구는 기계학습 기반으로 많이 이루어지고 있습니다.
이건 어떻게 보면 여러분에게 좋은 뉴스입니다. 여러분이 어떤 Background를 가지고 있든지 간에 여러분 관점에서 컴퓨터 비전을 해보실 수가 있다고 보실 수 있겠습니다.
그러면 지금부터는 Computer Vision의 소개를 마치고요. Computer Vision이 왜 어려운지.
제가 누누이 강조한 내용이 ‘Computer Vision이 재미는 있지만 어렵다.’ 이렇게 말씀을 드렸는데, 왜 어려운지에 대해서 하나하나씩 말씀을 드리도록 하겠습니다.
첫 번째 어려운 이유는 우리 환경은 3차원인데 사진이라는 건 3차원의 환경을 2차원으로, 어떻게 보면 정보의 손실을 감수하고 표현을 변화한 것입니다.
그래서 하나의 3차원 물체가 있을 때 여러분이 어느 방향에서 보느냐에 따라서 사진의 결과가 매우 달라집니다.
이건 재미있는 예로 미켈란질로의 유명한 작품인데, 심지어 사람에게 조차도 이 작품을 어떻게 찍었느냐에 따라서 이게 과연 똑같은 작품일까 의심이 될 정도로 매우 다르게 보입니다.
좀 전에 말씀드렸다시피 우리 환경은 3차원인데 그걸 사진으로 찍는 순간 2차원으로 변경함으로써 여러 가지 실제에서는 일어날 수 없는 현상이 사진 속에 이루어지게 됩니다.
위에서 보시면 어떤 이미지 상의 한 포인트는 카메라 센터로부터 그 이미지 plane의 포인트까지 직선을 그었을 때
그 직선에 해당하는 모든 포인트들이 그 이미지의 픽셀로 매핑이 될 수 있습니다.
그래서 occlusion, 가림 현상을 보시면, 이 사진에서 보시면 뒤에 건물 앞에 사람이 있음으로 해서 이 건물이 가려지게 되는데요.
그건 카메라 센터로부터 이미지 플레인까지 직선을 그었을 때 가장 먼저 만나는 환경 상의 지점이 사람의 다리였기 때문입니다.
그래서 제가 직선을 그으면 사람 다리를 거쳐서 건물까지 이어지지만, 그런데 사람 다리가 앞에 있음으로 해서 뒷부분이 보이지 않게 되는 것이죠.
이 사진 같은 경우에 마치 사람이 피사의 사탑을 발로 차는 것처럼 보이지만, 왜 이렇게 보이게 되는지 여러분이 약간이나마 이해하실 수 있을 것 같습니다.
그리고 오른쪽 이미지를 보시면, 사람은 매우 작고 호박이 엄청나게 큽니다. 그러면 사람은 이 사진을 보는 순간 ‘이렇게 큰 호박이 있구나.’ 이렇게 생각하지 않고요.
‘카메라의 아주 가까운 부분에 호박이 있고 사람은 멀리 있구나.’ 그러니까 멀리 있는 object는 조그맣게 보이고 가까이 있는 object는 크게 보입니다.
이것도 마찬가지로 3차원의 환경이 2차원으로 projection 되면서 벌어지는 현상이라고 보시면 되겠습니다.
두 번째로 어려운 점은 아까도 언뜻 말씀을 드렸지만, 우리가 어떤 물체를 본다는 것은 환경의 조명, 그러니까 광원이 존재했고요.
그 광원으로부터 나온 빛이 그 물체의 표면에서 반사를 일으켜 그 반사된 빛을 우리가 보기 때문입니다. 그래서 광원이 전혀 없으면 아무것도 우리는 볼 수 없게 되겠죠.
그래서 똑같은 사람이라고 하더라도 광원이 어디에 있느냐에 따라서 매우 다르게 보입니다. 이게 또 두 번째 Computer Vision 비전의 어려운 점이라고 할 수 있겠죠.
사진을 어떻게 보면 빛의 마술이라는 식으로 많이들 말씀하시는데, 그래서 이런 빛과 물체와의 interaction 때문에 아주 많은, 아주 이상한 여러 현상이 일어나게 됩니다.
왼쪽 그림을 보시면 이 부분은 모두 균일한 아스팔트라는 건 사람이 다 알고 있습니다. 그런데 이 부분과 이 부분을 비교해보시면 이 부분은 되게 밝고 이 부분은 되게 어둡습니다.
똑같은 영역임에도 불구하고 매우 다르게 보이는 거죠. 사람은 여기 나무가 보이지 않아도 ‘이게 그림자구나.’ 이걸 알 수 있지만,
기계는 이 사진을 보는 순간 ‘나무가 없는데 어떻게 그림자부터 보냐.’ 이렇게 이해할 수도 있다는 겁니다.
그리고 오른쪽 사진을 보면 이게 역광이 된 사진이기 때문에 역광에서 보면 물체는 항상 어둡게 보이고요.
왼쪽 그림과 마찬가지로 균일한 풀밭인데 어떤 부분은 어둡게, 어떤 부분은 밝게 보이게 되죠.
그리고 Illumination, 조명 조건으로 인해서 Computer Vision이 어려운 또 하나의 예는 이와 같습니다. 반사를 통해서 우리가 보는 물체와 현실이 다르다는 것이죠.
왼쪽 그림을 보시면, 이건 창문입니다. 여러분은 구름도 보실 수 있고, 나무도 보시고, 강도 보십니다.
이건 환경이 창문에 반사가 된 것인데, 기계 입장에서는 ‘여기에 나무가 있구나.’ 이렇게 이해합니다.
그러니까 창문을 보지 못하고 ‘창문에 비친 모습이다.’라고 이해하기보다는 ‘여기 나무가 있구나.’ 이렇게 생각하기 쉽다는 거고요.
그리고 오른쪽은 비가 오면 땅바닥에 중간 중간 물웅덩이가 있을 수 있고요. 그걸 통해서 환경이 반사가 될 수 있습니다.
그런데 여러분이 예를 들어서 자율주행차를 만드는데 자율주행 자동차의 사진기로부터 이런 사진을 얻었다. 그러면 갑자기 땅바닥에 나무가 보이는 겁니다.
그러면 자율주행 자동차가 갑자기 멈추게 될 텐데, 그럼 ‘이건 나무가 아니라 여기 물이 있었고 거기에 비친 거다.’ 이런 것들을 일일이 가르쳐준다는 게 참 어려운 일이라는 것입니다.
그리고 또 다른 여러 가지 물리적 광학 현상에 의해서 사진 해석이 어려운 점이 많은데요. 세 번째는 굴절입니다.
우리는 이 사진을 보는 순간 수영장 밑에 있는 타일은 정확한 사이즈로 규격화돼 있고 명확한 사각형이 균일하게 있다고 알지만, 이 사진은 모든 게 다 찌그러져 보이는 것이죠.
그리고 오른쪽 같은 경우에 사람의 경우에는 ‘이게 나뭇잎이고, 나뭇잎 뒤에 나뭇잎이 있다.’ 이런 것들을 알게 되겠지만,
반투명한 특성이 어디까지가 하나의 풀인지, 하나의 이파리인지 알아내는 게 매우 어렵게 됩니다.
그리고 그 외에 되게 다양한 자연현상이 있죠. 그러니까 안개가 낀다든지 비가 온다든지 혹은 눈이 내린다든지. 이 모든 게 다 연구 과제고요.
그래서 자율주행 자동차를 만들었는데 갑자기 눈이 많이 내려서 시야 확보가 안 될 때 어떻게 할 것이냐? 그런 어려움이 있고요.
그리고 실제로 비 오는 날에 찍은 이미지가 주어졌을 때 그 비 모습을 다 지워주는 기술조차도 제안이 되었습니다.
Computer Vision이라는 게 참 다양한 현상이 있고, 각각에 대해서 해결하기 위한 정말 다양한 연구가 진행되고 있다고 이해하실 수 있겠습니다.
그리고 세 번째 Challenge입니다. 세 번째 도전인데, 세 번째 도전인은 Occlusion, 그러니까 가림 현상이 있다는 것이죠.
좀 전에 왜 가림현상이 일어나는지 3차원의 세계를 2차원으로 projection 하면서 나타나는 현상이라고 말씀을 드렸는데요.
이 사진을 보는 순간 말 두 마리를 갈라져 있다고 생각하시는 분은 없을 겁니다. 사람에 의해서 말이 반으로 나눠진 것처럼 보인다.
이건 사진을 보는 순간 다 알 수 있는데, 이것조차도 컴퓨터에게 일일이 가르쳐 준다는 게 참 어렵다는 것이죠.
Occlusion이 왜 중요한지 단적으로 보여주는 사진입니다. 우리가 자율주행 자동차를 만든다고 할 때 보행자를 검출하는 것과 주변에 차를 인지하는 게 매우 중요한 작업입니다.
그런데 우리가 현실에서 사람이 돌아다니는 거리를 본다고 하면, 처음 머리부터 발끝까지 하나의 가려짐도 없이 완벽한 사람을 보는 경우는 매우 드물고요.
대다수의 사람은 어딘가가 가려져 있는 그런 사람들입니다. 그래서 이런 사진이 주어졌을 때 사람이 과연 몇 명 있느냐?
혹은 오른쪽 사진이 주어졌을 때 자동차가 몇 대 있느냐? 이걸 찾아낸다는 게 그렇게 쉬운 일은 아니겠죠.
그리고 이 부분이 아주 재미있는 부분인데, 사람은 아주 매우 작은 이 영역만 보고도 ‘거기에 차가 있구나.’를 알게 되는데,
만약에 기계가 그 부분만 봐서 이게 자동차처럼 생겼는지 아닌지로만 판단을 한다면 자동차라고 알 수가 없겠죠.
이게 자동차인지 알려면 도로라는 context가 있고, 버스는 되게 크고 그리고 버스의 이 정도 거리 뒤에 있는 뭔가의 물체는 자동차일 확률이 높구나.
이런 아주 복잡한 reasoning을 통해서 사람은 거기에 자동차가 있다는 걸 알게 됩니다.
네 번째 어려운 점은 Scale입니다. Scale이라는 건 똑같은 object라고 하더라도 이 이미지 상에서는 되게 크게 보일 수도 있고 아주 죽게 보일 수도 있다는 것입니다.
이 두 가지 object가 똑같은 object인지 알기가 어려울 수 있다는 의미고요. 예제와 함께 보도록 하겠습니다.
왼쪽 그림을 보시면 이건 일반적인 도로의 사진입니다. 그러면 저희가 이 도로의 사진 중에 두 가지 영역을 보도록 하죠. 이 두 가지 영역입니다.
우리가 이 사진을 보는 순간 이 영역에 사람이 있다는 건 아주 쉽게 알 수 있습니다. 그런데 저희가 만약에 사진만 놓고 봤을 때 이 영역을 잘라서 크게 확대하면 이와 같이 보입니다.
그래서 오른쪽 그림만 보고 이게 사람이구나, 아니구나 판단하는 건 너무 어려운 일이고요. 대신에 이렇게 작은 물체가 이미지 상에 존재하면 우리는 상대적으로 사람임을 쉽게 알 수 있습니다.
우리가 왜 사람인지 쉽게 알 수 있을까요? 우리는 context라는 정보를 활용합니다.
context라는 건 이 영역에 어떤 물체가 있는지를 판단하는 데 있어서 그 부분의 픽셀 값만 보는 게 아니라 주변 영역을 다 본다는 것이죠.
그래서 여기에 지금 땅바닥이 존재하고요. ground 플레인이 존재하고, 이 영역은 ground 플레인에 붙어 있다. 그러니까 걸어가는 사람일 것이다.
그리고 이 옆에 자동차가 하나 있는데 자동차와 사람 간에 상대적인 크기를 봤을 때 이 정도로 보이는 건 사람일 확률이 높고,
마찬가지로 옆에 큰 건물이 있는데 건물과 사람과의 크기 비율이 이 정도면 이 부분은 불명확하긴 하지만 사람이구나. 이런 걸 쉽게 알 수 있다는 겁니다.
그런데 기계에게 이런 것들을 일일이 가르쳐 주는 건 매우 힘들죠.
그러니까 여기에 사람이 있다는 걸 알기 위해서는 어디에 ground 플레인이 존재하고, 이 옆에 있는 게 자동차라는 걸 알아야 합니다.
그래서 많은 경우에 chicken and egg problem이 필요합니다.
그러니까 사람인지를 알려면 주변을 알아야 되고, 주변을 알려면 사람이 옆으로 걸어 다니는 상황이라는 걸 아는 게 매우 많이 도움이 되고요.
실제로 보행자 검출을 자율주행 자동차에서 한다고 할 때 사람이 명확하게 잘 보이는 건 거의 98% 이상으로 아주 정확하게 검출을 할 수 있습니다.
그런데 대다수의 실패 케이스는 언제냐 하면, 좀 전에 말씀드린 대로 occlusion이 매우 심하게 된 경우 혹은 매우 작게 보이는 경우입니다.
그리고 다섯 번째 어려운 점은 물체라는 게 하나의 형태만 가지고 있는 게 아니라 여러 가지 형태를 가지고 있다는 겁니다. 움직일 수 있고, 어렵게 deform이 가능하다는 거고요.
이 그림을 보시면, 지금 여기에 아주 많은 수의 말이 있습니다. 그런데 이 말들 중에 똑같이 보이는 말은 하나도 없고요. 똑같은 자세를 가진 말도 하나도 없습니다.
그러니까 강체라고 하죠? 자동차같이 딱딱한 물체 같은 경우에 shape, 그 모양이 정해져 있긴 하지만, 말 같은 경우에 관절이 있는 물체입니다.
그래서 관절이 있는 물체 같은 경우에 자기가 자기의 shape을 마음대로 바꿀 수가 있고요. 그리고 표면이 물렁물렁하기 때문에 deformation에 의해서 변화가 됩니다.
그리고 여섯 번째 Challenge는 Background, 그러니까 배경의 난잡함, 어려움 때문입니다. Background Clutter라고 많이 얘기하는데요.
여러분이 만약에 이미지 분류 작업을 한다고 할 때 무슨 쇼핑 사이트에서 상품 이미지, 깨끗이 찍힌 이미지를 분류하는 건 상대적으로 쉽지만,
만약에 그 물체가 일상 환경에 있었을 때 그걸 분류하는 건 어렵습니다.
왜냐하면, 일상 환경에서는 우리가 관심 있는 물체뿐만 아니라 여러 가지 다른 물체가 있기 때문에 그것 때문에 혼동을 얻게 된다는 것이죠.
그래서 지금 계속 예제와 함께 말씀을 드렸는데, 자율주행 자동차 예제를 한번 보도록 하죠.
이 사진은 대만의 한 거리에서 찍힌 사진입니다. 그리고 현재 자율주행 자동차가 가장 어려워하는 부분 중에 하나가 사인, 그러니까 신호등을 본다든지 아니면 교통 표지판을 보는 것입니다.
실제로 자동차에서 사진을 찍으면 신호등이 되게 조그맣게 보이고요. 여러 가지 사인 같은 것도 다 조그맣게 보입니다.
이 경우를 예로 들면, 지금 신호등이 파란 불이기 때문에 자동차가 갈 수 있는데, 보시다시피 신호등은 매우 조그맣게 보이고요.
그리고 배경에 수많은 Clutter들이 존재하기 때문에 과연 어디에 신호등이 있는지를 찾아내는 게 매우 어렵습니다.
그리고 이 특별한 예제에서는 간판 색깔로 빨간색이 많이 사용되고 있어서 만약 특정 영역을 신호등으로 잘못 인식하고 ‘여기 빨간 불이 있네?’라고 한다면, 자동차가 전혀 움직이지 못하겠죠.
그런 식으로 Background Clutter가 실제로 환경을 인식하는 데 있어서 매우 어려운 점이 됩니다.
그리고 마지막 Challenge에 대해서 말씀드리도록 하겠습니다. 이거는 무슨 뜻이냐 하면, 물체 내부의 Variation, 변화가 매우 크다는 것입니다.
여기에 지금 6가지의 사진이 있는데요. 이 6가지는 다 의자에 대한 사진입니다. 사람은 이것들을 보고 다 의자라고 생각하지만, 각각의 이미지, 의자의 겉모습은 매우 다릅니다.
사실 의자라는 물체를 정의하기 위해서는 ‘앉는다.’는 개념이 필요합니다.
그러니까 우리가 물체를 보고 ‘이건 의자다.’라고 생각하는 가장 큰 이유는 이 물체가 앉을 수 있느냐 없느냐를 따집니다.
그래서 이 사진을 보고 우리는 겉모습을 통해서 의자라는 걸 알아내는 게 아니라, 기능을 찾아보고 그걸 기반으로 ‘의자구나’ 이렇게 인식을 하게 됩니다.
그래서 이런 건 의자뿐만 아니라 되게 다양한 상황에서도 보일 수 있는데요.
여러분이 예를 들어 개와 고양이를 분류하고 싶다고 했을 때 개처럼 생긴 고양이도 되게 많고요. 그리고 고앙이처럼 생긴 개도 많습니다.
그래서 어떤 두 이미지는 둘 다 개인데 매우 다르게 생겼고, 이 개 이미지랑 고양이 이미지는 클래스가 다름에도 불구하고 되게 똑같이 보이는 그런 경우가 매우 많이 발생하게 됩니다.
지금까지 Computer Vision이 왜 어려운지 7개 Challenge에 대해서 하나하나씩 살펴봤고요. 어떻게 보면 너무 비관적인 말씀만 드린 것 같긴 한데요.
이러이러한 어려움이 있긴 하지만 근래에는 아주 많은 Computer Vision의 진보가 있어서 여러 분야에서 점점 Computer Vision의 활용이 커지고 있습니다.
그럼에도 불구하고 아직 텍스트 인식만큼 기계가 자동으로 사진을 인식하거나 동영상을 인식하는 상황은 이루어지지 않았다고 보시면 좋겠고요.
그래서 Computer Vision분야에서 해야 될 일이 매우 많다고 이해하시면 되겠습니다.
시간이 갈수록 Computer Vision이란 학문의 중요성은 점점 커지고 있습니다.
한 가지 예를 들어보면, 여러분이 요즘 인터넷을 통해서 가장 많이 하는 활동을 보시면 많은 경우에 사진을 보거나 동영상을 보는 경우입니다.
그러니까 인터넷 트래픽 관점에서 보면, 95% 이상, 압도적인 트래픽이 동영상을 전송하거나 사진을 전송하는 경우죠.
결국에 온라인상에 Computer Vision으로 해석해야지만 되는 정보량이 점점 많아지고 있습니다. 그러니까 텍스트 정보의 증가 못지않게 시각 정보의 양이 아주 급격히 늘어나고 있고요.
현재 기술로는 많은 시각 정보가 제대로 해석되지 않고 그냥 버려져 있는 혹은 그 시각 정보 주변에 쓰여 있는 텍스트 태그를 기반으로 해서 검색을 한다든지 이해를 하는 그런 경우가 많은데요.
인공지능 기술이 계속 발전하다 보면, 사진과 동영상을 기계가 바로 이해해서 그거로부터 사람처럼 행동할 수 있는 날이 머지않을 거라고 생각합니다.
그래서 지금 8개의 강의를 통해서 인공지능의 기초에 대해서 말씀을 드렸습니다. 마지막으로 Computer Vision에 대해서 말씀을 드렸고요.
어떻게 보면 소개만 말씀을 드렸고, ‘이러이러한 점에서 어렵다.’는 점만 말씀을 드렸는데요. 여러분이 이 강의를 통해서 흥미를 가지시고 ‘이런 문제는 풀 수 있겠구나.’
그리고 기사에서 보듯이 인공지능이라는 게 다 되는, 금방이라도 될 것 같은 쉬운 문제만은 아니고 아직도 해결해야 될 문제가 많다. 그렇기 때문에 더 공부를 해야겠다.
이런 생각을 가지게 되셨으면 아주 좋을 것 같습니다. 그러면 마지막 강의를 여기서 마치도록 하겠습니다. 그동안 경청해 주셔서 감사합니다.
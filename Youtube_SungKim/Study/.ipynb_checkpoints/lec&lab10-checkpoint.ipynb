{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-01\n",
    "\n",
    "## 이번 강의의 핵심은 Backpropagation의 등장 이후 NN엔 Vanishing Gradient라는 문제가 발생했는데 이걸 어떻게 해결 했는지를 다뤄본다.\n",
    "\n",
    "### Hinton은 Vanishg Gradient를 아래와 같은 4가지 관점에서 봤다.\n",
    "\n",
    "### 그중 첫번째는 Activation Function에서 Sigmoid를 버리고 ReLU를 도입 한것이다.\n",
    "\n",
    "오늘은 좀 깊게 들어가볼거다\n",
    "\n",
    "우리가 계속 쓰던 Sigmoid 같은걸 Activation function이라고 많이 부름\n",
    "\n",
    "한 유닛을 활성화 시킬지 말지 돌려주기 때문이지.\n",
    "\n",
    "\n",
    "3중첩, 4중첩 등등 하는 방법은 쉬움\n",
    "\n",
    "위 코드 처럼 shape만 잘 맞춰주면서 쌓으면 됨.\n",
    "\n",
    "여러 겹이 쌓여있을때 가장 안쪽을 Input Layer, 가장 바깥쪽을 Output Layer, 중간을 Hidden Layer라고 부름\n",
    "\n",
    "여러 겹 쌓을때 중요한건 input shape과 output shape을 잘 정해주는 거임.\n",
    "\n",
    "중간 shape들은 주고 받을 수만 있으면 어떻게 하든 상관 없음.\n",
    "\n",
    "\n",
    "그리고 앞으론 텐서보드로 시각화 할걸 대비해서 코드를 짜주면 좋음\n",
    "\n",
    "위에 있는 코드처럼.\n",
    "\n",
    "\n",
    "복잡한 신경망을 구성할수록 이런 시각화 전략은 중요함. \n",
    "\n",
    "\n",
    "강의 예제를 돌려보면 9겹을 쌓았는데도 정확도가 구림. 이럴때 텐서보드를 보면서 원인을 찾아보는 거임.\n",
    "\n",
    "왜 이런 문제가 발생할까.\n",
    "\n",
    "\n",
    "1986년도에 Backpropagation이 발명됐을때 사람들 매우 좋아했음.\n",
    "\n",
    "근데, 2~3단은 잘 학습이 됐는데 이런 방식으로 단순하게 만들면 9~10단 같이 깊은 곳 까진 학습이 안되는 것임.\n",
    "\n",
    "왜 그럴까 알아보자.\n",
    "\n",
    "Sigmoid를 거치면 일단 1 이하 값이 나옴. 그런데 Sigmoid로 들어가는 값이 음수로 많이 작아지면 0.00... 이런식으로 많이 작아짐. 근데 이게 Backpropagation 에서 Chain rule을 거치며 너무 작아지는 거임.\n",
    "\n",
    "이걸 좀 멋있는 말로 Vanishing gradient 문제라고함. 바깥쪽 layer의 gradient는 학습 할 만큼 충분한데\n",
    "\n",
    "뒤로 갈수록 vanishing 된다는 거임.\n",
    "\n",
    "이 문제 때문에 NN은 1986~2006의 긴 기간동안 침체기에 빠짐.\n",
    "\n",
    "\n",
    "이것도 암튼 깨졌음. Hinton에 따르면 우리가 Sigmoid를 쓰면 안됐다는 거임.\n",
    "\n",
    "Sigmoid의 문제는 항상 1보다 작은 값을 내는데 이게 계속 곱해지는게 문제라는 거임.\n",
    "\n",
    "\n",
    "그래서 나온게 ReLU임. f(x) = max(0, x) 의 매우 간단한 함수임. 위 그래프 처럼.\n",
    "\n",
    "ReLU : Rectified Linear Unit\n",
    "\n",
    "그래서 이제 NN에선 Sigmoid안쓴다고 보면됨. ReLU쓰면됨\n",
    "\n",
    "Sigmoid는 가장 마지막 레이어에서만 쓰면 됨. 가장 마지막 레이어에선 0~1값을 내야 하기 때문에. \n",
    "\n",
    "\n",
    "아까 썼던 예제 코드를 ReLU를 이용하면 쉽게 풀림. \n",
    "\n",
    "\n",
    "\n",
    "ReLU는 아주 간단했지. 그래서 사람들이 ReLU 를 이용해 비슷한 함수들을 많이 만듬. \n",
    "\n",
    "위 슬라이드 처럼 많은 ReLU들이 나옴. \n",
    "\n",
    "\n",
    "tanh는 Sigmoid를 살려보기 위해 만든거.\n",
    "\n",
    "\n",
    "암튼 이제 Sigmoid는 마지막 레이어 아니면 볼일 없다. \n",
    "\n",
    "ReLU를 비롯한 다른 ReLU들은 공부해보고 잘 골라쓰면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-02\n",
    "\n",
    "### 두번째로 알아볼건 W의 초기값 설정이다.\n",
    "\n",
    "위 그래프를 보자. 같은 코드로 ReLU를 돌렸는데 Cost가 떨어지는 속도가 좀 차이난다. \n",
    "\n",
    "이건 현재 W를 랜덤값으로 줬기 때문이다.\n",
    "\n",
    "초기값에 따리 이렇게 학습 속도가 차이나는 것이다. \n",
    "\n",
    "\n",
    "만약 초기 W가 0이 들어가면 어떻게 될까. 위 그림 처럼 변화율이 다 0이 되버릴거다.\n",
    "\n",
    "\n",
    "\n",
    "암튼 그래서 딥한 네트워크에서 초기값 설정이 중요하다.\n",
    "\n",
    "2006년에 Hinton이 RBM 이란걸 처음 내놓음. 지금은 잘 쓰진 않지만 자주 나오는 용어기 때문에 알아두자.\n",
    "\n",
    "RBM을 사용해 초기값 설정한 걸 DBN이라고 부름.\n",
    "\n",
    "\n",
    "RBM은 어떻게 쓰는걸까. 우선 RBM에서 Restricted 라는선 레이어 안에서는 연결이 없고 앞뒤로만 연결 됐다는 거임.\n",
    "\n",
    "하는 방법은 일단 평범하게 Forward를 보냄. 그리고 Backward 를 보낼땐 위의 그림처럼 Forward할때 썼던 w를 이용해 뒤를 보냄. \n",
    "\n",
    "그렇게 해서 보냈을때의 x와 Backward로 나온 x'의 차이가 최소가 되도록 만드는 거임. \n",
    "\n",
    "이걸 Encode / Decode라고도 부름\n",
    "\n",
    "\n",
    "기본 아이디어는ㄴ 이거고 이걸로 초기값을 설정하는거임\n",
    "\n",
    "인접한 2개의 레이어 끼리 저 주고받는걸 진행해서 w를 설정하는 거임. \n",
    "\n",
    "이 과정을 pre-training 라고도 부르고 Fine Tuning 라고도 부름. 위 사진 처럼 전체 NN을 보는게 아니라 인접한 두 레이어끼리만 Forward, Backward하면서 w를 셋팅함. \n",
    "\n",
    "구현하려면 조금 복잡해보이긴 하는데... 아까 요즘은 이걸 안쓴다고 했지. \n",
    "\n",
    "\n",
    "위 슬라이드에 있는거 처럼 요즘엔 2010년에 나온 Xavier initialization 방법을 쓰기도 하고 2015년엔 저게 개선된 방법인 He initialization을 쓰기도 함.\n",
    "\n",
    "두 방법의 기본은 fan in과 fan out 을 가지고 구하는 거임. 위 코드 처럼.<br>\n",
    "fan in/fan out은 그냥 input/output이라고 보면 됨.\n",
    "\n",
    "근데 알렉스넷을 만들었던 사람이 2015년에 위 코드처럼 fan in을 2로 나누면 엄청 잘된다는 거임 ㅋㅋ\n",
    "\n",
    "이 분야는 뭐 연금술 같이 수식을 만드나\n",
    "\n",
    "\n",
    "사실 이 초기값 설정의 분야는 아직 활발히 연구되는 분야임.\n",
    "\n",
    "그래서 적당히 방법들 써가면서 자기 모델에 잘 맞는거 쓰면 됨\n",
    "\n",
    "\n",
    "이렇게 위 슬라이드에서 제시된 4가지중 2가지를 알아봤음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-3\n",
    "\n",
    "이 강의에선 Drop out과 앙상블을 알아본다.\n",
    "\n",
    "전에 오버 피팅에 대해 얘기한적이 있다. \n",
    "\n",
    "너무 트레이닝 데이터에 꽉 맞는 모델이 나오는거.\n",
    "\n",
    "오버피팅의 증상은 1. 트레이닝 데이터론 0.99 같은 매우 높은 정확성이 나오는데 2. 테스트 데이터론 0.85 같이 낮은 정확도가 나옴.\n",
    "\n",
    "\n",
    "오버피팅은 레이어를 많이 쓸수록 잘 나타남.\n",
    "\n",
    "오버피팅을 줄이는 방법\n",
    "\n",
    "1. 테스트 데이터 수를 늘림\n",
    "2. feature의 수를 줄임 -> 딥 러닝에선 이건 할 필요 없음\n",
    "3. Regularization\n",
    "\n",
    "여기서 알아볼 방법은 Regularization이지.\n",
    "\n",
    "구불 거리던 cost를 좀 펴주는 거였고\n",
    "\n",
    "방법은 위 슬라이드 처럼 Regularization strength를 붙이는 거였지\n",
    "\n",
    "여기서 중요한건 람다 값. 람다를 얼마나 주느냐에 따라 Regularization을 얼마나 세게줄지를 조정 가능\n",
    "\n",
    "\n",
    "NN에선 Regularization의 또 다른 방법이 있음. 바로 Dropout이지.\n",
    "\n",
    "방법은 황당할 정도로 간단함. 위 처럼 랜덤하게 몇개 뉴런을 끊는 거임. \n",
    "\n",
    "\n",
    "이게 왜 잘 먹히는가는 사실 만든사람도 모르고 그냥 의미를 부여 해보자면 위와 같은거다.\n",
    "\n",
    "고양이를 맞추는 모델을 만들때 각각 특성을 인지하는 유닛을 모두 계속 활성화 시키면 오버피팅이 발생하는거고\n",
    "\n",
    "학습시에 몇몇 특성들은 쉬게 했다가 학습 끝나고 실제 run 할때 모든 특성을 다같이 쓰면 잘 맞춘다는 거임.\n",
    "\n",
    "구현은 위와 같이 쓰면 간담함. tf.nn에 있는 dropout을 쓰면 되고 dropout_rate만 잘 조정하면 됨. dropout 0.7은 0.3은 쉬고 0.7만 쓴다는 거임.\n",
    "\n",
    "\n",
    "여기서 중요한건 학습 할때만 drop 하는거임. 실전에선 모두 다 불러와야지. \n",
    "\n",
    "이 실전에서 모두다 불러올땐 dropout_rate 1을 주면 됨.\n",
    "\n",
    "\n",
    "마지막으로 앙상블을 알아봄. \n",
    "\n",
    "같은 모델을 여러개를 만들어 초기 값이나 그런 것들을 조금씩 다르게 줌.\n",
    "\n",
    "그리고 각각 모델을 학습시키고 실전에선 여러개를 섞는거임.\n",
    "\n",
    "약간 여러 전문가들이 각각 학습하고 같이 모여서 실전 푸는거임.\n",
    "\n",
    "\n",
    "2~5%까지 성능 향상이 된다고 함. \n",
    "\n",
    "잘 먹히는 모델임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-04\n",
    "\n",
    "이제 원하는 대로 모델들을 쌓기만 하면 됨. \n",
    "\n",
    "근데 이게 약간 레고 같음. \n",
    "\n",
    "지금 까지 배운 이렇게 층층이 쌓는걸 FeedForward Neural Network라고함.\n",
    "\n",
    "\n",
    "근데 다른 형태도 많다.\n",
    "\n",
    "Fast Forward는 위 그림처럼 갑자기 몇몇 유닛을 건너뛰고 데이터를 보내기도 하는거임. 이게 2015년에 이미지넷에 오류를 3% 아래로 떨어뜨린 방법임\n",
    "\n",
    "Split Merge 방법도 있음. 위 그림처럼 말 그대로 split 했다가 merge 하는 거임. \n",
    "\n",
    "위 그림의 아래 네트워크는 Split, merge를 이용한 그 유명한 Convolutional Network임\n",
    "\n",
    "\n",
    "Recurrent network는 위 그림처럼 만드는 거임. 이건 RNN이라고 부르는 언어쪽에서 유명한 네트워크임. 전의 데이터도 써야되는 네트워크. 문맥 파악 같은거\n",
    "\n",
    "상상력을 많이 밣휘 하여라!!\n",
    "\n",
    "비디오 여기까지 봤으면 거의 하산 준비 하여라.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번 강의에선 NN 하면서 쓸 수 있는 꿀팁들을 위주로 알려주겠다. \n",
    "\n",
    "# 이 곳까지 왔다면 우리 깃에 협업해도 좋다. \n",
    "\n",
    "# MNIST 를 다시 끌고와보자. \n",
    "\n",
    "# lab 7에서 했던 MNIST를 8,9,10에서 배운 것들을 이용해 좀 업그레이드 시켜봄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래는 전 까지 썼던 코드임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이걸 더 깊게 구성해봄 전엔 그냥 Softmax 라고 부를 수 있고 이렇게 여러개를 쌓아야 Neural Net 이라고 부를 수 있다.\n",
    "\n",
    "# NN에서 중간 shape는 원하는거 잘 골라서 쓰면 된다.\n",
    "\n",
    "# input shape이랑 output shape만 잘 맞춰주자. \n",
    "\n",
    "# 여기선 3단 layer + ReLU를 써보자.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 전엔 0.90 정도 였는덴 0.94 까지 올라감."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기선 초기화를 더 잘 해보자. xvaier를 써보자. \n",
    "\n",
    "# 쓰는 방법은 구글링하면 잘 나옴. \n",
    "\n",
    "\n",
    "\n",
    "# 0.9783 까지 올라갔다. 와우...\n",
    "\n",
    "# 근데 이번 결과를 다른 결과랑 비교해보면 처음부터 cost가 낮다는걸 확인할 수 있음.\n",
    "\n",
    "# 이게 초기값 설정의 중요성임. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한번 더 깊고 넓게 만들어보자.\n",
    "\n",
    "\n",
    "\n",
    "# 근데 결과는...? 물론 높은 값이지만 크게 향상되지 않았네 오히려 0.004 줄음. \n",
    "\n",
    "# 이렇게 이슈가 발생하면 이것저것 써가면서 이유를 찾아봐야 하는데 아마 여기선 오버피팅이 문제가 아닐까 싶다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오버피팅이 문제였다면! Dropout을 한번 써보자\n",
    "\n",
    "# 구현은 dropout 레이어를 넣어주면 된다.\n",
    "\n",
    "# 텐서플로 1.0에선 dropout_rate가 아니라 keep_prob으로 바뀜. 얼마나 keep 할거냐는 더 직관적인 변수명이지. \n",
    "\n",
    "# 학습할땐 keep_prob을 0.5~0.7 주고 테스트 할땐 1.0 주면 된다. \n",
    "\n",
    "# keep_prob도 placeholder로 구성해두고 학습할때, 테스트 할때 다른 값을 주면 됨. \n",
    "\n",
    "\n",
    "\n",
    "# 결과는... 0.9804!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가로 강의에선 알려주지 않았지만 Optimizer를 바꿔볼 수도 있다.\n",
    "\n",
    "# 텐서플로 안에도 여러 Optimizer가 있으니까 설명 읽어보고 잘 맞는거 써보면 된다. \n",
    "\n",
    "# 교수님이 조사한 자료로는 Adam이라는 Optimizer가 성능이 제일 좋네.\n",
    "\n",
    "# 그래서 잘 모르면 그냥 Adam 써봐라.교수님 권장 사항임. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lab을 요약해보자면 아래 슬라이드와 같음\n",
    "\n",
    "\n",
    "여러 꿀팁들을 추가할수록 Accuracy를 높일 수 있다. \n",
    "\n",
    "그리고 여기서 나오지 않았지만 Batch Normalization이라는 방법도 있다. 입력값의 Normalize를 잘 하는 방법임. \n",
    "\n",
    "다음시간은 더 나가보자 CNN이라는걸 알아볼거임. Convoulutional Neural Network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
